# NewsCrawlerTool(BaseTool)
"""
NewsCrawlerTool - ë‰´ìŠ¤ í¬ë¡¤ë§ ë„êµ¬

GoogleNews ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì—¬ëŸ¬ ì†ŒìŠ¤ì—ì„œ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
"""
from gnews import GNews
from typing import Dict, List, Any, Optional
from datetime import datetime, timedelta
import time
import asyncio
from concurrent.futures import ThreadPoolExecutor
from src.tools.base.base_tool import BaseTool
from src.tools.base.tool_config import ToolConfig
from src.core.models.citation_model import NewsCitation


class NewsCrawlerTool(BaseTool):
    """
    ë‰´ìŠ¤ í¬ë¡¤ë§ ë„êµ¬
    
    Features:
    - í‚¤ì›Œë“œ ê¸°ë°˜ ë‰´ìŠ¤ ê²€ìƒ‰
    - ë‚ ì§œ ë²”ìœ„ í•„í„°ë§
    - ì—¬ëŸ¬ ì†ŒìŠ¤ì—ì„œ ìˆ˜ì§‘
    - ì¤‘ë³µ ì œê±°
    - Rate limiting ìë™ ì²˜ë¦¬
    
    Example:
        tool = NewsCrawlerTool(config)
        result = tool._run(
            keywords=["humanoid robot", "embodied AI"],
            date_range="3 years",
            sources=5
        )
    """
    
    def __init__(self, config: ToolConfig):
        super().__init__(config)
        # GNews í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.gnews = GNews(
            language='en',
            country='US',  # ì „ì„¸ê³„ ê²€ìƒ‰ì„ ìœ„í•´ ë‚˜ì¤‘ì— ë³€ê²½í•  ìˆ˜ë„ ìˆìŒ
            period='7d',  # ê¸°ë³¸ê°’ (ì‹¤ì œë¡œëŠ” date_rangeë¡œ ë®ì–´ì”€)
            max_results=100,  # í‚¤ì›Œë“œë‹¹ ìµœëŒ€ ê²°ê³¼
            exclude_websites=[]  # ì œì™¸í•  ì‚¬ì´íŠ¸ ì—†ìŒ
        )
    
    def _run(
        self,
        keywords: List[str],
        date_range: str = "3 years",
        sources: int = 5
    ) -> Dict[str, Any]:
        """
        ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤í–‰
        
        Args:
            keywords: ê²€ìƒ‰ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
            date_range: ê²€ìƒ‰ ë‚ ì§œ ë²”ìœ„
                - "3 years": ìµœê·¼ 3ë…„
                - "36 months": ìµœê·¼ 36ê°œì›”
                - "90 days": ìµœê·¼ 90ì¼
            sources: í¬ë¡¤ë§í•  ë‰´ìŠ¤ ì†ŒìŠ¤ ê°œìˆ˜ (1-5)
        
        Returns:
            {
                "keywords": List[str],
                "date_range": str,
                "total_articles": int,
                "unique_sources": int,
                "articles": [
                    {
                        "title": str,
                        "url": str,
                        "source": str,
                        "published": str,  # YYYY-MM-DD
                        "snippet": str
                    },
                    ...
                ]
            }
        """
        # ì¬ì‹œë„ ë¡œì§
        for attempt in range(self.config.retry_count):
            try:
                # 1. ë‚ ì§œ ë²”ìœ„ë¥¼ GNews period í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                period = self._parse_date_range(date_range)
                self.gnews.period = period
                
                # 2. ê° í‚¤ì›Œë“œë³„ë¡œ ë‰´ìŠ¤ ìˆ˜ì§‘
                all_articles = []
                for i, keyword in enumerate(keywords, 1):
                    print(f"\n   [{i}/{len(keywords)}] Processing keyword: '{keyword}'")
                    articles = self._fetch_keyword(keyword, sources)
                    all_articles.extend(articles)
                    
                    # Rate limiting ë°©ì§€ (í‚¤ì›Œë“œ ê°„ ì§§ì€ ëŒ€ê¸°)
                    if i < len(keywords):
                        time.sleep(0.5)  # 1ì´ˆ â†’ 0.5ì´ˆë¡œ ë‹¨ì¶•
                
                # 3. ì¤‘ë³µ ì œê±° (URL ê¸°ì¤€)
                unique_articles = self._deduplicate(all_articles)
                
                # 4. ë‚ ì§œ ë²”ìœ„ í•„í„°ë§
                filtered_articles = self._filter_by_date(unique_articles, date_range)
                
                # 5. ë‚ ì§œ ìˆœìœ¼ë¡œ ì •ë ¬ (ìµœì‹ ìˆœ)
                filtered_articles.sort(key=lambda x: x['published'], reverse=True)
                
                # 6. ì†ŒìŠ¤ ê°œìˆ˜ ê³„ì‚°
                unique_sources = len(set(article['source'] for article in filtered_articles))
                
                # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
                print(f"\n   [News] Collection Summary:")
                print(f"       Total collected: {len(all_articles)} articles")
                print(f"       After deduplication: {len(unique_articles)} articles")
                print(f"       After date filter: {len(filtered_articles)} articles")
                print(f"       Unique sources: {unique_sources}")
                
                # Citations ìƒì„±
                citations = []
                for article in filtered_articles:
                    try:
                        citation = NewsCitation(
                            title=article.get("title", "Unknown"),
                            source=article.get("source", "Unknown"),
                            published=article.get("published", "Unknown"),
                            url=article.get("url", "")
                        )
                        citations.append(citation)
                    except Exception as e:
                        print(f"       Citation ìƒì„± ì‹¤íŒ¨: {str(e)[:100]}")
                        continue
                
                print(f"       Citations created: {len(citations)}")
                
                return {
                    "keywords": keywords,
                    "date_range": date_range,
                    "total_articles": len(filtered_articles),
                    "unique_sources": unique_sources,
                    "articles": filtered_articles,
                    "citations": citations  # Citation ê°ì²´ ë¦¬ìŠ¤íŠ¸ ì¶”ê°€
                }
            
            except Exception as e:
                if attempt < self.config.retry_count - 1:
                    # ì¬ì‹œë„ ì „ ëŒ€ê¸°
                    time.sleep(2 ** attempt)  # Exponential backoff
                    continue
                else:
                    # ìµœì¢… ì‹¤íŒ¨
                    return {
                        "keywords": keywords,
                        "date_range": date_range,
                        "total_articles": 0,
                        "unique_sources": 0,
                        "articles": [],
                        "error": f"Failed after {self.config.retry_count} attempts: {str(e)}"
                    }
    
    def _fetch_keyword(self, keyword: str, sources: int) -> List[Dict[str, Any]]:
        """
        ë‹¨ì¼ í‚¤ì›Œë“œë¡œ ë‰´ìŠ¤ ìˆ˜ì§‘ (ë” ë§ì€ ê¸°ì‚¬ ìˆ˜ì§‘)
        
        Args:
            keyword: ê²€ìƒ‰ í‚¤ì›Œë“œ
            sources: ëª©í‘œ ì†ŒìŠ¤ ê°œìˆ˜ (ë¬´ì‹œí•˜ê³  ìµœëŒ€í•œ ë§ì´ ìˆ˜ì§‘)
        
        Returns:
            ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
        """
        articles = []
        
        try:
            print(f"   [News] Searching keyword: '{keyword}'")
            
            # GNews ê²€ìƒ‰
            results = self.gnews.get_news(keyword)
            
            if not results:
                print(f"   [News] No results for '{keyword}'")
                return []
            
            print(f"   [News] Found {len(results)} raw results for '{keyword}'")
            
            # ì†ŒìŠ¤ë³„ë¡œ ê·¸ë£¹í™”
            source_dict = {}
            for result in results:
                source = result.get('publisher', {}).get('title', 'Unknown')
                if source not in source_dict:
                    source_dict[source] = []
                source_dict[source].append(result)
            
            # ëª¨ë“  ì†ŒìŠ¤ì—ì„œ ê¸°ì‚¬ ìˆ˜ì§‘ (sources íŒŒë¼ë¯¸í„° ë¬´ì‹œ)
            # ê° ì†ŒìŠ¤ì—ì„œ ìµœëŒ€ 10ê°œì”© ìˆ˜ì§‘í•˜ì—¬ ë‹¤ì–‘ì„± í™•ë³´
            for source, source_articles in source_dict.items():
                # í•´ë‹¹ ì†ŒìŠ¤ì—ì„œ ìµœëŒ€ 10ê°œ ê¸°ì‚¬ ìˆ˜ì§‘
                for article in source_articles[:10]:
                    try:
                        formatted = self._format_article(article)
                        articles.append(formatted)
                    except Exception as format_error:
                        print(f"   [News] Failed to format article: {format_error}")
                        continue
            
            print(f"   [News] Collected {len(articles)} articles from {len(source_dict)} sources for '{keyword}'")
            
            return articles
        
        except Exception as e:
            print(f"   [News] Failed to fetch news for keyword '{keyword}': {e}")
            import traceback
            print(f"   ğŸ“œ [News] Traceback: {traceback.format_exc()}")
            return []
    
    def _format_article(self, raw_article: Dict) -> Dict[str, Any]:
        """
        GNews ê²°ê³¼ë¥¼ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        
        Args:
            raw_article: GNews ì›ë³¸ ê²°ê³¼
        
        Returns:
            í‘œì¤€í™”ëœ ê¸°ì‚¬ ë”•ì…”ë„ˆë¦¬
        """
        # ë°œí–‰ì¼ íŒŒì‹±
        published_date = raw_article.get('published date', '')
        try:
            # "Mon, 15 Jan 2024 12:34:56 GMT" í˜•ì‹
            dt = datetime.strptime(published_date, "%a, %d %b %Y %H:%M:%S %Z")
            published_str = dt.strftime("%Y-%m-%d")
        except:
            published_str = datetime.now().strftime("%Y-%m-%d")
        
        return {
            "title": raw_article.get('title', ''),
            "url": raw_article.get('url', ''),
            "source": raw_article.get('publisher', {}).get('title', 'Unknown'),
            "published": published_str,
            "snippet": raw_article.get('description', '')
        }
    
    def _deduplicate(self, articles: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        URL ê¸°ì¤€ ì¤‘ë³µ ì œê±°
        
        Args:
            articles: ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
        
        Returns:
            ì¤‘ë³µ ì œê±°ëœ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
        """
        seen_urls = set()
        unique_articles = []
        
        for article in articles:
            url = article['url']
            if url not in seen_urls:
                seen_urls.add(url)
                unique_articles.append(article)
        
        return unique_articles
    
    def _filter_by_date(
        self,
        articles: List[Dict[str, Any]],
        date_range: str
    ) -> List[Dict[str, Any]]:
        """
        ë‚ ì§œ ë²”ìœ„ë¡œ í•„í„°ë§
        
        Args:
            articles: ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
            date_range: "3 years", "36 months", "90 days"
        
        Returns:
            í•„í„°ë§ëœ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
        """
        # ë‚ ì§œ ë²”ìœ„ íŒŒì‹±
        cutoff_date = self._calculate_cutoff_date(date_range)
        
        filtered = []
        for article in articles:
            try:
                article_date = datetime.strptime(article['published'], "%Y-%m-%d")
                if article_date >= cutoff_date:
                    filtered.append(article)
            except:
                # ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨ ì‹œ í¬í•¨
                filtered.append(article)
        
        return filtered
    
    def _parse_date_range(self, date_range: str) -> str:
        """
        date_rangeë¥¼ GNews period í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        
        Args:
            date_range: "3 years", "36 months", "90 days"
        
        Returns:
            GNews period í˜•ì‹ (ì˜ˆ: "3y", "36m", "90d")
        """
        date_range = date_range.lower().strip()
        
        # ìˆ«ì ì¶”ì¶œ
        parts = date_range.split()
        if len(parts) != 2:
            return "7d"  # ê¸°ë³¸ê°’
        
        number = parts[0]
        unit = parts[1]
        
        # ë‹¨ìœ„ ë³€í™˜
        if 'year' in unit:
            return f"{number}y"
        elif 'month' in unit:
            return f"{number}m"
        elif 'day' in unit:
            return f"{number}d"
        else:
            return "7d"  # ê¸°ë³¸ê°’
    
    def _calculate_cutoff_date(self, date_range: str) -> datetime:
        """
        date_rangeë¡œë¶€í„° cutoff ë‚ ì§œ ê³„ì‚°
        
        Args:
            date_range: "3 years", "36 months", "90 days"
        
        Returns:
            cutoff datetime
        """
        date_range = date_range.lower().strip()
        parts = date_range.split()
        
        if len(parts) != 2:
            return datetime.now() - timedelta(days=7)  # ê¸°ë³¸ê°’
        
        number = int(parts[0])
        unit = parts[1]
        
        now = datetime.now()
        if 'year' in unit:
            return now - timedelta(days=365 * number)
        elif 'month' in unit:
            return now - timedelta(days=30 * number)
        elif 'day' in unit:
            return now - timedelta(days=number)
        else:
            return now - timedelta(days=7)  # ê¸°ë³¸ê°’
    
    async def search_by_keywords_parallel(
        self,
        keywords: List[str],
        date_range: str = "3 years"
    ) -> Dict[str, Any]:
        """
        í‚¤ì›Œë“œë³„ ë³‘ë ¬ ë‰´ìŠ¤ ìˆ˜ì§‘
        
        Args:
            keywords: ê²€ìƒ‰ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
            date_range: ë‚ ì§œ ë²”ìœ„
        
        Returns:
            {
                "articles": [...],
                "count": int,
                "keywords_searched": int
            }
        """
        print(f"\në³‘ë ¬ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘: {len(keywords)}ê°œ í‚¤ì›Œë“œ")
        
        # ThreadPoolExecutorë¡œ ë³‘ë ¬ ìˆ˜ì§‘
        loop = asyncio.get_event_loop()
        with ThreadPoolExecutor(max_workers=5) as executor:
            tasks = [
                loop.run_in_executor(
                    executor,
                    self._search_single_keyword,
                    keyword,
                    date_range
                )
                for keyword in keywords[:10]  # ìµœëŒ€ 10ê°œ
            ]
            
            results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ê²°ê³¼ ë³‘í•©
        all_articles = []
        seen_urls = set()
        
        for result in results:
            if isinstance(result, Exception):
                print(f"   âš ï¸ ì—ëŸ¬ ë°œìƒ: {result}")
                continue
            
            if isinstance(result, list):
                for article in result:
                    url = article.get('url', '')
                    if url and url not in seen_urls:
                        seen_urls.add(url)
                        all_articles.append(article)
        
        # ë‚ ì§œìˆœ ì •ë ¬
        all_articles.sort(
            key=lambda x: datetime.strptime(x['published'], "%Y-%m-%d") if x.get('published') else datetime.min,
            reverse=True
        )
        
        print(f"   ì´ {len(all_articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ (ì¤‘ë³µ ì œê±° í›„)")
        
        # Citations ìƒì„±
        citations = []
        for article in all_articles:
            try:
                citation = NewsCitation(
                    title=article.get("title", "Unknown"),
                    source=article.get("source", "Unknown"),
                    published=article.get("published", "Unknown"),
                    url=article.get("url", "")
                )
                citations.append(citation)
            except Exception as e:
                print(f"   Citation ìƒì„± ì‹¤íŒ¨: {str(e)[:100]}")
                continue
        
        print(f"   Citations created: {len(citations)}")
        
        return {
            "articles": all_articles,
            "citations": citations,  # Citation ê°ì²´ ë¦¬ìŠ¤íŠ¸ ì¶”ê°€
            "count": len(all_articles),
            "keywords_searched": len(keywords)
        }
    
    def _search_single_keyword(
        self,
        keyword: str,
        date_range: str
    ) -> List[Dict[str, Any]]:
        """
        ë‹¨ì¼ í‚¤ì›Œë“œë¡œ ë‰´ìŠ¤ ìˆ˜ì§‘ (ë™ê¸° í•¨ìˆ˜)
        
        Args:
            keyword: ê²€ìƒ‰ í‚¤ì›Œë“œ
            date_range: ë‚ ì§œ ë²”ìœ„
        
        Returns:
            ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
        """
        print(f"   '{keyword}' ìˆ˜ì§‘ ì¤‘...")
        
        try:
            # GNews period ì„¤ì •
            period = self._parse_date_range(date_range)
            gnews = GNews(
                language='en',
                country='US',
                period=period,
                max_results=10  # í‚¤ì›Œë“œë‹¹ 10ê°œ
            )
            
            # ê²€ìƒ‰
            articles = gnews.get_news(keyword)
            
            # í˜•ì‹ ë³€í™˜
            formatted = []
            for article in articles:
                # ë‚ ì§œ íŒŒì‹±
                published_date = article.get('published date', '')
                try:
                    # "Mon, 15 Jan 2024 12:34:56 GMT" í˜•ì‹
                    dt = datetime.strptime(published_date, "%a, %d %b %Y %H:%M:%S %Z")
                    published_str = dt.strftime("%Y-%m-%d")
                except:
                    # íŒŒì‹± ì‹¤íŒ¨ ì‹œ í˜„ì¬ ë‚ ì§œ
                    published_str = datetime.now().strftime("%Y-%m-%d")
                
                formatted.append({
                    'title': article.get('title', ''),
                    'description': article.get('description', ''),
                    'url': article.get('url', ''),
                    'published': published_str,  # ë³€í™˜ëœ ë‚ ì§œ ì‚¬ìš©
                    'source': article.get('publisher', {}).get('title', 'Unknown'),
                    'keyword': keyword
                })
            
            print(f"      âœ“ '{keyword}': {len(formatted)}ê°œ")
            return formatted
            
        except Exception as e:
            print(f"      âœ— '{keyword}' ì‹¤íŒ¨: {e}")
            return []
    
    async def _arun(self, *args, **kwargs):
        """ë¹„ë™ê¸° ì‹¤í–‰ (LangChain í˜¸í™˜)"""
        raise NotImplementedError("NewsCrawlerTool does not support async execution")

