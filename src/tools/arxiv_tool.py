"""
ArxivTool - arXiv ë…¼ë¬¸ ê²€ìƒ‰ ë„êµ¬ (ê°œì„  ë²„ì „)

ê°œì„ ì‚¬í•­:
1. í˜ì´ì§• ì—ëŸ¬ ì‹œ ìˆ˜ì§‘í•œ ë°ì´í„° ë³´ì¡´
2. 500ê°œ ì œí•œ (ì¶©ë¶„í•œ ë°ì´í„° ìˆ˜ì§‘)
3. ë¶€ë¶„ ì„±ê³µ ë°˜í™˜ ê¸°ëŠ¥
4. ë” ë‚˜ì€ ì—ëŸ¬ í•¸ë“¤ë§

ë³€ê²½ ì´ë ¥:
- v1.0: ì´ˆê¸° ë²„ì „
- v1.1: í˜ì´ì§• ì—ëŸ¬ ëŒ€ì‘, ë¶€ë¶„ ê²°ê³¼ ë°˜í™˜ ì¶”ê°€
- v1.2: ì œí•œì„ 100 â†’ 500ìœ¼ë¡œ ì¦ê°€
"""
import arxiv
import re
from typing import Dict, List, Any, Optional, Set
from datetime import datetime, timedelta
import time
import asyncio
from concurrent.futures import ThreadPoolExecutor
from src.tools.base.base_tool import BaseTool
from src.tools.base.tool_config import ToolConfig
from src.core.models.citation_model import ArXivCitation


class ArxivTool(BaseTool):
    """
    arXiv ë…¼ë¬¸ ê²€ìƒ‰ ë„êµ¬
    
    Features:
    - í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰
    - ë‚ ì§œ ë²”ìœ„ í•„í„°ë§
    - ì¹´í…Œê³ ë¦¬ í•„í„°ë§
    - ìµœëŒ€ ê²°ê³¼ ê°œìˆ˜ ì œí•œ (500ê°œê¹Œì§€)
    - í˜ì´ì§• ì—ëŸ¬ ì‹œ ë¶€ë¶„ ê²°ê³¼ ë°˜í™˜
    - ê¸°ì—…ëª… ì¶”ì¶œ (ë…¼ë¬¸ì—ì„œ ì–¸ê¸‰ëœ ê¸°ì—…)
    - í‚¤ì›Œë“œ ì¶”ì¶œ (ì œëª©/ì´ˆë¡ ê¸°ë°˜)
    
    Example:
        tool = ArxivTool(config)
        result = tool._run(
            keywords=["humanoid robot", "manufacturing"],
            date_range="2022-01-01 to 2025-10-22",
            categories="cs.RO,cs.AI",
            max_results="500"
        )
    """
    
    # ì£¼ìš” AI/ë¡œë´‡ ê¸°ì—… ë¦¬ìŠ¤íŠ¸ (ëŒ€ì†Œë¬¸ì ë¬´ì‹œ)
    COMPANIES = {
        # ë¹…í…Œí¬
        "Google", "Meta", "Microsoft", "Apple", "Amazon", "Tesla", "NVIDIA",
        "DeepMind", "OpenAI", "Anthropic",
        
        # ë¡œë´‡ ì „ë¬¸
        "Boston Dynamics", "Figure AI", "Agility Robotics", "ANYbotics",
        "Universal Robots", "ABB Robotics", "KUKA", "FANUC", "Yaskawa",
        
        # ìë™ì°¨
        "Toyota", "BMW", "Mercedes", "Waymo", "Cruise", "Ford", "GM",
        
        # ì¤‘êµ­ ë¹…í…Œí¬
        "Baidu", "Alibaba", "Tencent", "ByteDance", "Huawei",
        
        # ê¸°íƒ€
        "SpaceX", "Neuralink", "iRobot", "Fetch Robotics", "Rethink Robotics"
    }
    
    def __init__(self, config: ToolConfig):
        super().__init__(config)
        # arxiv Client ì„¤ì • (ë³´ìˆ˜ì )
        self.client = arxiv.Client(
            page_size=50,       # í˜ì´ì§€ë‹¹ 50ê°œ
            delay_seconds=4,    # ìš”ì²­ ê°„ 4ì´ˆ ëŒ€ê¸° (rate limit íšŒí”¼)
            num_retries=2       # ì¬ì‹œë„ 2íšŒ
        )
        
        # ê¸°ì—…ëª… ë§¤ì¹­ìš© ì •ê·œì‹ íŒ¨í„´ ìƒì„±
        self.company_patterns = [
            re.compile(r'\b' + re.escape(company) + r'\b', re.IGNORECASE)
            for company in self.COMPANIES
        ]
    
    def _run(
        self,
        keywords: List[str],
        date_range: str,
        categories: str = "all",
        max_results: str = "unlimited"
    ) -> Dict[str, Any]:
        """
        arXiv ë…¼ë¬¸ ê²€ìƒ‰ ì‹¤í–‰
        
        Args:
            keywords: ê²€ìƒ‰ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
            date_range: ë‚ ì§œ ë²”ìœ„ (ì˜ˆ: "2022-01-01 to 2025-10-22")
            categories: arXiv ì¹´í…Œê³ ë¦¬ (all ë˜ëŠ” "cs.RO,cs.AI")
            max_results: ìµœëŒ€ ê²°ê³¼ ê°œìˆ˜ (unlimited ë˜ëŠ” ìˆ«ì)
                - unlimited: 100ê°œë¡œ ì œí•œ (arXiv API í•œê³„)
                - ìˆ«ì: í•´ë‹¹ ê°œìˆ˜ë§Œí¼ (ìµœëŒ€ 100)
        
        Returns:
            {
                "total_count": int,
                "papers": [
                    {
                        "title": str,
                        "authors": List[str],
                        "abstract": str,
                        "url": str,
                        "published": str,
                        "categories": List[str]
                    },
                    ...
                ],
                "warning": str (optional, ë¶€ë¶„ ê²°ê³¼ì¸ ê²½ìš°)
            }
        """
        # ì¬ì‹œë„ ë¡œì§ ë°–ì—ì„œ best_papers ê´€ë¦¬
        best_papers = []
        
        # ì¬ì‹œë„ ë¡œì§
        for attempt in range(self.config.retry_count):
            try:
                # 1. ì¿¼ë¦¬ ìƒì„±
                query = self._build_query(keywords, categories)
                
                # 2. ë‚ ì§œ ë²”ìœ„ íŒŒì‹±
                start_date, end_date = self._parse_date_range(date_range)
                
                # 3. max_results íŒŒì‹± (500ê°œ ì œí•œ)
                if max_results == "unlimited":
                    max_results_int = 500  # ì¶©ë¶„í•œ ë°ì´í„° ìˆ˜ì§‘
                else:
                    max_results_int = min(int(max_results), 500)
                
                print(f"ğŸ” Searching arXiv: max_results={max_results_int}")
                if attempt > 0:
                    print(f"   (Retry {attempt}/{self.config.retry_count})")
                
                # 4. arXiv ê²€ìƒ‰
                search = arxiv.Search(
                    query=query,
                    max_results=max_results_int,
                    sort_by=arxiv.SortCriterion.SubmittedDate,
                    sort_order=arxiv.SortOrder.Descending
                )
                
                # 5. ê²°ê³¼ ìˆ˜ì§‘ (ì˜ˆì™¸ ì²˜ë¦¬ ì¶”ê°€)
                papers = []
                count = 0
                filtered_out = 0
                start_time = time.time()
                
                print(f"   Starting paper collection...")
                print(f"   ğŸ“… Date filter: {start_date} to {end_date}")
                
                try:
                    for result in self.client.results(search):
                        count += 1
                        
                        # ì•ˆì „ ì¥ì¹˜: max_results_int ê°œìˆ˜ë§Œí¼ ìˆ˜ì§‘í•˜ë©´ ì¤‘ë‹¨
                        if count >= max_results_int:
                            print(f"   âœ… Reached {max_results_int} papers, stopping...")
                            break
                        
                        # ì§„í–‰ ìƒí™© í‘œì‹œ (50ê°œë§ˆë‹¤)
                        if count % 50 == 0:
                            elapsed = time.time() - start_time
                            print(f"   ğŸ“„ Processed {count} papers, collected {len(papers)} (filtered out: {filtered_out}) - {elapsed:.1f}s")
                        
                        # ë‚ ì§œ í•„í„°ë§
                        try:
                            published_date = result.published.date()
                            if start_date <= published_date <= end_date:
                                # ê¸°ì—…ëª… ì¶”ì¶œ (ì œëª© + ì´ˆë¡)
                                text_to_analyze = f"{result.title} {result.summary}"
                                companies = self._extract_companies(text_to_analyze)
                                
                                # ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ (ì œëª© ê¸°ë°˜)
                                extracted_keywords = self._extract_keywords(result.title, result.summary)
                                
                                paper = {
                                    "title": result.title,
                                    "authors": [author.name for author in result.authors],
                                    "abstract": result.summary,
                                    "url": result.entry_id,
                                    "published": published_date.strftime("%Y-%m-%d"),
                                    "categories": result.categories,
                                    "companies": companies,  # ì–¸ê¸‰ëœ ê¸°ì—…
                                    "keywords": extracted_keywords  # ì¶”ì¶œëœ í‚¤ì›Œë“œ
                                }
                                papers.append(paper)
                            else:
                                filtered_out += 1
                                if filtered_out == 1 or filtered_out % 20 == 0:
                                    print(f"   â­ï¸  Filtered out {filtered_out} papers (outside date range)")
                        except Exception as paper_error:
                            print(f"   âš ï¸  Skipping paper: {paper_error}")
                            continue
                
                except Exception as fetch_error:
                    # í˜ì¹­ ì¤‘ ì—ëŸ¬ ë°œìƒ - ì§€ê¸ˆê¹Œì§€ ìˆ˜ì§‘í•œ papersëŠ” ìœ ì§€
                    print(f"   âš ï¸  Fetch interrupted at {count} papers: {fetch_error}")
                    print(f"   ğŸ“¦ Saving {len(papers)} papers collected so far...")
                    print(f"   ğŸ“Š Stats: processed={count}, collected={len(papers)}, filtered_out={filtered_out}")
                
                # ìˆ˜ì§‘í•œ papers ì—…ë°ì´íŠ¸
                if len(papers) > len(best_papers):
                    best_papers = papers
                
                elapsed_total = time.time() - start_time
                print(f"   âœ… Collected {len(papers)} papers (from {count} processed, filtered out: {filtered_out}, {elapsed_total:.1f}s)")
                
                # papersê°€ ìˆìœ¼ë©´ ì„±ê³µìœ¼ë¡œ ê°„ì£¼
                if len(papers) > 0:
                    # ê¸°ì—… í†µê³„ ìƒì„±
                    company_stats = self._generate_company_stats(papers)
                    
                    result = {
                        "total_count": len(papers),
                        "papers": papers,
                        "company_stats": company_stats  # ê¸°ì—…ë³„ ë…¼ë¬¸ ìˆ˜
                    }
                    
                    # ë¶€ë¶„ ê²°ê³¼ì¸ ê²½ìš° ê²½ê³  ì¶”ê°€
                    if count < max_results_int:
                        result["warning"] = f"Partial results: collected {count}/{max_results_int} papers"
                    
                    # ê¸°ì—… í†µê³„ ì¶œë ¥
                    if company_stats:
                        top_companies = sorted(company_stats.items(), key=lambda x: x[1], reverse=True)[:5]
                        print(f"   ğŸ¢ Top companies: {', '.join([f'{c}({n})' for c, n in top_companies])}")
                    
                    print(f"âœ… ArXiv search successful: {len(papers)} papers")
                    return result
                
                # papersê°€ 0ê°œë©´ ì¬ì‹œë„
                raise ValueError(f"No papers collected (processed {count})")
            
            except Exception as e:
                print(f"âŒ ArXiv search failed (attempt {attempt + 1}/{self.config.retry_count}): {e}")
                
                if attempt < self.config.retry_count - 1:
                    wait_time = 5 * (2 ** attempt)  # 5s, 10s, 20s...
                    print(f"â³ Waiting {wait_time}s before retry...")
                    time.sleep(wait_time)
                    continue
                else:
                    # ìµœì¢… ì‹¤íŒ¨ - best_papersë¼ë„ ë°˜í™˜
                    if len(best_papers) > 0:
                        print(f"ğŸ’¡ Returning {len(best_papers)} papers from previous attempts")
                        return {
                            "total_count": len(best_papers),
                            "papers": best_papers,
                            "warning": "Partial results from retry attempts"
                        }
                    else:
                        print(f"ğŸ’¥ No papers collected after {self.config.retry_count} attempts")
                        return {
                            "total_count": 0,
                            "papers": [],
                            "error": f"Failed after {self.config.retry_count} attempts: {str(e)}"
                        }
    
    def _build_query(self, keywords: List[str], categories: str) -> str:
        """
        arXiv ì¿¼ë¦¬ ë¬¸ìì—´ ìƒì„±
        
        Args:
            keywords: ê²€ìƒ‰ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ (ìµœëŒ€ 10ê°œ ê¶Œì¥)
            categories: ì¹´í…Œê³ ë¦¬ (all ë˜ëŠ” "cs.RO,cs.AI")
        
        Returns:
            arXiv API ì¿¼ë¦¬ ë¬¸ìì—´
        
        Note:
            í‚¤ì›Œë“œê°€ ë„ˆë¬´ ë§ìœ¼ë©´ ì¿¼ë¦¬ê°€ ë³µì¡í•´ì ¸ API ì—ëŸ¬ ë°œìƒ ê°€ëŠ¥
            ê¶Œì¥: 10ê°œ ì´í•˜ (callerì—ì„œ ì œí•œí•´ì•¼ í•¨)
        """
        # í‚¤ì›Œë“œê°€ ë„ˆë¬´ ë§ìœ¼ë©´ ê²½ê³ 
        if len(keywords) > 10:
            print(f"   âš ï¸  Warning: {len(keywords)} keywords provided (recommended: â‰¤10)")
            print(f"   âš ï¸  Complex queries may cause ArXiv API errors")
        
        # í‚¤ì›Œë“œë¥¼ ORë¡œ ì—°ê²°
        keyword_query = " OR ".join([f'all:"{kw}"' for kw in keywords])
        
        # ì¹´í…Œê³ ë¦¬ í•„í„°
        if categories != "all":
            cat_list = [cat.strip() for cat in categories.split(",")]
            cat_query = " OR ".join([f'cat:{cat}' for cat in cat_list])
            return f"({keyword_query}) AND ({cat_query})"
        
        return keyword_query
    
    def _parse_date_range(self, date_range: str) -> tuple:
        """
        ë‚ ì§œ ë²”ìœ„ íŒŒì‹±
        
        Args:
            date_range: "2022-01-01 to 2025-10-22" í˜•ì‹
        
        Returns:
            (start_date, end_date) tuple
        
        Raises:
            ValueError: ë‚ ì§œ í˜•ì‹ì´ ì˜ëª»ëœ ê²½ìš°
        """
        try:
            parts = date_range.lower().split(" to ")
            if len(parts) != 2:
                raise ValueError(f"Invalid date_range format: {date_range}")
            
            start_date = datetime.strptime(parts[0].strip(), "%Y-%m-%d").date()
            end_date = datetime.strptime(parts[1].strip(), "%Y-%m-%d").date()
            
            return start_date, end_date
        
        except ValueError as e:
            raise ValueError(
                f"date_range must be in format 'YYYY-MM-DD to YYYY-MM-DD': {e}"
            )
    
    def search_by_keywords_parallel(
        self,
        keywords: List[str],
        categories: str = "all",
        max_results_per_keyword: int = 100,
        years_back: int = 3
    ) -> Dict[str, Any]:
        """
        ê° í‚¤ì›Œë“œë³„ë¡œ ê°œë³„ ê²€ìƒ‰ì„ ë³‘ë ¬ë¡œ ìˆ˜í–‰ (ìµœì‹  ë…¼ë¬¸ ìœ„ì£¼)
        
        Args:
            keywords: ê²€ìƒ‰ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
            categories: arXiv ì¹´í…Œê³ ë¦¬
            max_results_per_keyword: í‚¤ì›Œë“œë‹¹ ìµœëŒ€ ê²°ê³¼ ìˆ˜
            years_back: ëª‡ ë…„ ì „ë¶€í„° ê²€ìƒ‰í• ì§€ (ê¸°ë³¸: 3ë…„)
        
        Returns:
            ì¤‘ë³µ ì œê±°ëœ ë…¼ë¬¸ ê²°ê³¼
        """
        print(f"\nğŸ” Starting parallel search for {len(keywords)} keywords")
        print(f"   ğŸ“… Date range: Last {years_back} years")
        print(f"   ğŸ“Š Max per keyword: {max_results_per_keyword}")
        
        # ë‚ ì§œ ë²”ìœ„ ê³„ì‚° (ìµœê·¼ Në…„)
        end_date = datetime.now()
        start_date = end_date - timedelta(days=years_back * 365)
        date_range = f"{start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}"
        
        # ThreadPoolExecutorë¥¼ ì‚¬ìš©í•˜ì—¬ ë³‘ë ¬ ê²€ìƒ‰
        all_papers = []
        seen_ids = set()
        
        with ThreadPoolExecutor(max_workers=5) as executor:
            # ê° í‚¤ì›Œë“œë³„ë¡œ ê²€ìƒ‰ ì‘ì—… ì œì¶œ
            futures = []
            for i, keyword in enumerate(keywords):
                print(f"   ğŸ”¸ Submitting search {i+1}/{len(keywords)}: '{keyword}'")
                future = executor.submit(
                    self._search_single_keyword,
                    keyword=keyword,
                    date_range=date_range,
                    categories=categories,
                    max_results=max_results_per_keyword,
                    keyword_index=i+1,
                    total_keywords=len(keywords)
                )
                futures.append(future)
            
            # ê²°ê³¼ ìˆ˜ì§‘ ë° ì¤‘ë³µ ì œê±°
            for future in futures:
                try:
                    result = future.result()
                    if result and result.get("papers"):
                        for paper in result["papers"]:
                            paper_id = paper["url"].split("/")[-1]  # arXiv ID ì¶”ì¶œ
                            if paper_id not in seen_ids:
                                seen_ids.add(paper_id)
                                all_papers.append(paper)
                except Exception as e:
                    print(f"   âš ï¸  Keyword search failed: {e}")
        
        # ë°œí–‰ì¼ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ìµœì‹ ìˆœ)
        all_papers.sort(key=lambda p: p["published"], reverse=True)
        
        # ê¸°ì—… í†µê³„ ìƒì„±
        company_stats = self._generate_company_stats(all_papers)
        
        print(f"\nâœ… Parallel search complete!")
        print(f"   ğŸ“š Total unique papers: {len(all_papers)}")
        print(f"   ğŸ”„ Duplicates removed: {sum(len(f.result().get('papers', [])) for f in futures if f.done()) - len(all_papers)}")
        
        # ê¸°ì—… í†µê³„ ì¶œë ¥
        if company_stats:
            top_companies = sorted(company_stats.items(), key=lambda x: x[1], reverse=True)[:5]
            print(f"   ğŸ¢ Top companies: {', '.join([f'{c}({n})' for c, n in top_companies])}")
        
        # Citations ìƒì„±
        citations = []
        for paper in all_papers:
            try:
                url = paper.get("url", "")
                arxiv_id = url.split("/")[-1] if url else "unknown"
                
                citation = ArXivCitation(
                    authors=paper.get("authors", []),
                    title=paper.get("title", "Unknown"),
                    arxiv_id=arxiv_id,
                    published=paper.get("published", "Unknown"),
                    url=url
                )
                citations.append(citation)
            except Exception as e:
                print(f"   âš ï¸  Citation ìƒì„± ì‹¤íŒ¨: {str(e)[:100]}")
                continue
        
        print(f"   ğŸ“š Citations created: {len(citations)}")
        
        return {
            "total_count": len(all_papers),
            "papers": all_papers,
            "citations": citations,  # Citation ê°ì²´ ë¦¬ìŠ¤íŠ¸ ì¶”ê°€
            "company_stats": company_stats,  # ê¸°ì—…ë³„ ë…¼ë¬¸ ìˆ˜
            "search_method": "parallel_by_keyword",
            "keywords_searched": len(keywords),
            "date_range": date_range
        }
    
    def _search_single_keyword(
        self,
        keyword: str,
        date_range: str,
        categories: str,
        max_results: int,
        keyword_index: int,
        total_keywords: int
    ) -> Dict[str, Any]:
        """
        ë‹¨ì¼ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰ (ë‚´ë¶€ í—¬í¼ ë©”ì„œë“œ)
        
        Args:
            keyword: ê²€ìƒ‰ í‚¤ì›Œë“œ
            date_range: ë‚ ì§œ ë²”ìœ„
            categories: ì¹´í…Œê³ ë¦¬
            max_results: ìµœëŒ€ ê²°ê³¼ ìˆ˜
            keyword_index: í˜„ì¬ í‚¤ì›Œë“œ ì¸ë±ìŠ¤
            total_keywords: ì „ì²´ í‚¤ì›Œë“œ ìˆ˜
        
        Returns:
            ê²€ìƒ‰ ê²°ê³¼
        """
        print(f"\n   [{keyword_index}/{total_keywords}] ğŸ” Searching: '{keyword}'")
        
        try:
            # ê¸°ì¡´ _run ë©”ì„œë“œ ì¬ì‚¬ìš© (ë‹¨ì¼ í‚¤ì›Œë“œ)
            result = self._run(
                keywords=[keyword],  # ë‹¨ì¼ í‚¤ì›Œë“œ
                date_range=date_range,
                categories=categories,
                max_results=str(max_results)
            )
            
            papers_count = len(result.get("papers", []))
            print(f"   [{keyword_index}/{total_keywords}] âœ… Found {papers_count} papers for '{keyword}'")
            
            return result
        
        except Exception as e:
            print(f"   [{keyword_index}/{total_keywords}] âŒ Failed for '{keyword}': {e}")
            return {"papers": [], "total_count": 0}
    
    def _extract_companies(self, text: str) -> List[str]:
        """
        ë…¼ë¬¸ í…ìŠ¤íŠ¸ì—ì„œ ê¸°ì—…ëª… ì¶”ì¶œ
        
        Args:
            text: ë¶„ì„í•  í…ìŠ¤íŠ¸ (ì œëª© + ì´ˆë¡)
        
        Returns:
            ë°œê²¬ëœ ê¸°ì—…ëª… ë¦¬ìŠ¤íŠ¸
        """
        found_companies = []
        text_lower = text.lower()
        
        for company in self.COMPANIES:
            # ëŒ€ì†Œë¬¸ì ë¬´ì‹œí•˜ê³  ë§¤ì¹­
            if company.lower() in text_lower:
                # ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•´ ì •ê·œí™”ëœ ì´ë¦„ ì‚¬ìš©
                if company not in found_companies:
                    found_companies.append(company)
        
        return found_companies
    
    def _extract_keywords(self, title: str, abstract: str) -> List[str]:
        """
        ë…¼ë¬¸ ì œëª©ê³¼ ì´ˆë¡ì—ì„œ ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ
        
        ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜ ì¶”ì¶œ:
        - ëŒ€ë¬¸ìë¡œ ì‹œì‘í•˜ëŠ” ì—°ì†ëœ ë‹¨ì–´ (ê¸°ìˆ  ìš©ì–´)
        - í•˜ì´í”ˆìœ¼ë¡œ ì—°ê²°ëœ ë‹¨ì–´
        - íŠ¹ì • íŒ¨í„´ì˜ ê¸°ìˆ  ìš©ì–´
        
        Args:
            title: ë…¼ë¬¸ ì œëª©
            abstract: ë…¼ë¬¸ ì´ˆë¡
        
        Returns:
            ì¶”ì¶œëœ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ (ìµœëŒ€ 10ê°œ)
        """
        keywords = []
        
        # ì£¼ìš” ë¡œë´‡/AI ê¸°ìˆ  í‚¤ì›Œë“œ íŒ¨í„´
        tech_patterns = [
            r'\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+){1,3}\b',  # "Reinforcement Learning", "Deep Neural Network"
            r'\b\w+(?:-\w+)+\b',  # "Multi-Agent", "End-to-End"
            r'\b(?:AI|ML|RL|DRL|CNN|RNN|LSTM|GAN|VAE|NLP|CV)\b',  # ì•½ì–´
        ]
        
        # ì œëª©ê³¼ ì´ˆë¡ì˜ ì¼ë¶€ë§Œ ë¶„ì„ (ì„±ëŠ¥ ê³ ë ¤)
        text_to_analyze = f"{title} {abstract[:500]}"
        
        for pattern in tech_patterns:
            matches = re.findall(pattern, text_to_analyze)
            keywords.extend(matches)
        
        # ì¤‘ë³µ ì œê±° ë° ì •ì œ
        keywords = list(set(keywords))
        
        # ë¶ˆìš©ì–´ ì œê±° (ë„ˆë¬´ ì¼ë°˜ì ì¸ ë‹¨ì–´)
        stopwords = {"The", "This", "These", "Our", "We", "In", "On", "For", "To", "From"}
        keywords = [kw for kw in keywords if kw not in stopwords]
        
        # ìµœëŒ€ 10ê°œë§Œ ë°˜í™˜ (ë¹ˆë„ìˆœìœ¼ë¡œ ì •ë ¬í•˜ë©´ ë” ì¢‹ì§€ë§Œ, ê°„ë‹¨í•˜ê²Œ)
        return keywords[:10]
    
    def _generate_company_stats(self, papers: List[Dict]) -> Dict[str, int]:
        """
        ë…¼ë¬¸ ë¦¬ìŠ¤íŠ¸ì—ì„œ ê¸°ì—…ë³„ í†µê³„ ìƒì„±
        
        Args:
            papers: ë…¼ë¬¸ ë¦¬ìŠ¤íŠ¸ (ê° ë…¼ë¬¸ì— "companies" í•„ë“œ í¬í•¨)
        
        Returns:
            {ê¸°ì—…ëª…: ë…¼ë¬¸ ìˆ˜} ë”•ì…”ë„ˆë¦¬
        """
        company_counts = {}
        
        for paper in papers:
            companies = paper.get("companies", [])
            for company in companies:
                company_counts[company] = company_counts.get(company, 0) + 1
        
        return company_counts
    
    async def _arun(self, *args, **kwargs):
        """ë¹„ë™ê¸° ì‹¤í–‰ (LangChain í˜¸í™˜)"""
        raise NotImplementedError("ArxivTool does not support async execution")


