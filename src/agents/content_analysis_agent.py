"""
Content Analysis Agent (LCEL ë°©ì‹)

ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ íŠ¸ë Œë“œ ë¶„ë¥˜, ì„¹ì…˜ ìƒì„±, ì¸ìš© ê´€ë¦¬ë¥¼ ìˆ˜í–‰í•˜ëŠ” Agent
LCELì„ ì‚¬ìš©í•œ 4ë²ˆì˜ ë…ë¦½ì ì¸ LLM í˜¸ì¶œ
"""
import json
import asyncio
from typing import List, Any, Dict
from langchain_core.language_models import BaseChatModel
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.runnables import RunnablePassthrough

from src.agents.base.base_agent import BaseAgent
from src.agents.base.agent_config import AgentConfig
from src.graph.state import PipelineState
from src.core.models.trend_model import TrendTier
from src.core.models.citation_model import CitationEntry
from config.prompts.analysis_prompts import ANALYSIS_PROMPTS


class ContentAnalysisAgent(BaseAgent):
    """
    Content Analysis Agent (LCEL ë°©ì‹)
    
    ìˆ˜ì§‘ëœ ë°ì´í„° ë¶„ì„ ë° ë³´ê³ ì„œ ë‚´ìš© ìƒì„±
    
    Workflow (4ë²ˆ LLM í˜¸ì¶œ):
    1. ë³‘ë ¬: Section 2 + Section 3
    2. ìˆœì°¨: Section 4 (Section 2, 3 ê¸°ë°˜)
    3. ìˆœì°¨: Section 5 (Section 2, 3, 4 ê¸°ë°˜)
    
    Responsibilities:
    1. ë°ì´í„° í†µí•© ë¶„ì„ (arXiv, Trends, News, RAG)
    2. íŠ¸ë Œë“œ í‹°ì–´ ë¶„ë¥˜ (HOT_TRENDS, RISING_STARS)
    3. ì„¹ì…˜ë³„ ë‚´ìš© ìƒì„± (10ê°œ ì„œë¸Œì„¹ì…˜)
       - 2.1~2.2: ê¸°ìˆ  íŠ¸ë Œë“œ ë¶„ì„
       - 3.1~3.3: ì‹œì¥ ë™í–¥ ë° ì‚°ì—… ì ìš©
       - 4.1~4.2: 5ë…„ ê¸°ìˆ  ì „ë§
       - 5.1~5.3: ê¸°ì—… ì‹œì‚¬ì 
    4. ì¸ìš© ê´€ë¦¬ (CitationEntry)
    
    Output:
    - trends: List[TrendTier]
    - sections: Dict[str, str] (10ê°œ ì„œë¸Œì„¹ì…˜)
    - citations: List[CitationEntry]
    """
    
    def __init__(
        self,
        llm: BaseChatModel,
        tools: List[Any],
        config: AgentConfig
    ):
        super().__init__(llm, tools, config)
        self._setup_chains()
    
    def _setup_chains(self):
        """LCEL Chains ì„¤ì •"""
        # Output parser
        json_parser = JsonOutputParser()
        
        # Section 2 Chain
        section_2_prompt = ChatPromptTemplate.from_messages([
            ("system", ANALYSIS_PROMPTS["system"]),
            ("human", ANALYSIS_PROMPTS["section_2"])
        ])
        self.section_2_chain = section_2_prompt | self.llm | json_parser
        
        # Section 3 Chain
        section_3_prompt = ChatPromptTemplate.from_messages([
            ("system", ANALYSIS_PROMPTS["system"]),
            ("human", ANALYSIS_PROMPTS["section_3"])
        ])
        self.section_3_chain = section_3_prompt | self.llm | json_parser
        
        # Section 4 Chain
        section_4_prompt = ChatPromptTemplate.from_messages([
            ("system", ANALYSIS_PROMPTS["system"]),
            ("human", ANALYSIS_PROMPTS["section_4"])
        ])
        self.section_4_chain = section_4_prompt | self.llm | json_parser
        
        # Section 5 Chain
        section_5_prompt = ChatPromptTemplate.from_messages([
            ("system", ANALYSIS_PROMPTS["system"]),
            ("human", ANALYSIS_PROMPTS["section_5"])
        ])
        self.section_5_chain = section_5_prompt | self.llm | json_parser
    
    async def execute(self, state: PipelineState) -> PipelineState:
        """
        Content Analysis ì‹¤í–‰ (LCEL ë°©ì‹)
        
        Args:
            state: í˜„ì¬ íŒŒì´í”„ë¼ì¸ ìƒíƒœ
                - state["planning_output"]: PlanningOutput
                - state["arxiv_data"]: arXiv ë°ì´í„°
                - state["trends_data"]: Trends ë°ì´í„°
                - state["news_data"]: News ë°ì´í„°
                - state["rag_results"]: RAG ê²°ê³¼
                - state["expanded_keywords"]: í™•ì¥ëœ í‚¤ì›Œë“œ
        
        Returns:
            ì—…ë°ì´íŠ¸ëœ ìƒíƒœ
                - state["trends"]: List[TrendTier]
                - state["sections"]: Dict[str, str] (10ê°œ ì„œë¸Œì„¹ì…˜)
                - state["citations"]: List[CitationEntry]
        """
        print(f"\n{'='*60}")
        print(f"ğŸ”¬ Content Analysis Agent ì‹¤í–‰ ì¤‘ (LCEL ë°©ì‹)...")
        print(f"{'='*60}\n")
        
        # Stateì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        topic = state.get("planning_output").topic
        keywords = state.get("expanded_keywords", [])
        arxiv_data = state.get("arxiv_data", {})
        trends_data = state.get("trends_data", {})
        news_data = state.get("news_data", {})
        rag_results = state.get("rag_results", {})
        
        try:
            # Step 1: ë°ì´í„° ìš”ì•½ ìƒì„±
            print(f"ğŸ“Š Step 1: ë°ì´í„° ìš”ì•½ ìƒì„± ì¤‘...")
            data_summaries = self._create_data_summaries(
                arxiv_data, trends_data, news_data, rag_results
            )
            print(f"   âœ… ìš”ì•½ ìƒì„± ì™„ë£Œ\n")
            
            # Step 2 & 3: Section 2, 3 ë³‘ë ¬ ì‹¤í–‰
            print(f"ğŸ”€ Step 2 & 3: Section 2, 3 ë³‘ë ¬ ìƒì„± ì¤‘...")
            section_2_result, section_3_result = await self._run_parallel_sections(
                topic, keywords, data_summaries
            )
            print(f"   âœ… Section 2 ì™„ë£Œ (trends: {len(section_2_result.get('trends', []))}ê°œ)")
            print(f"   âœ… Section 3 ì™„ë£Œ (sections: {len(section_2_result.get('sections', {}))}ê°œ)\n")
            
            # Step 4: Section 4 ìˆœì°¨ ì‹¤í–‰
            print(f"â¡ï¸  Step 4: Section 4 ìƒì„± ì¤‘ (Section 2, 3 ê¸°ë°˜)...")
            section_4_result = await self._run_section_4(
                topic, section_2_result, section_3_result, data_summaries
            )
            print(f"   âœ… Section 4 ì™„ë£Œ\n")
            
            # Step 5: Section 5 ìˆœì°¨ ì‹¤í–‰
            print(f"â¡ï¸  Step 5: Section 5 ìƒì„± ì¤‘ (Section 2, 3, 4 ê¸°ë°˜)...")
            section_5_result = await self._run_section_5(
                topic, section_2_result, section_3_result, section_4_result
            )
            print(f"   âœ… Section 5 ì™„ë£Œ\n")
            
            # Step 6: ê²°ê³¼ í†µí•© ë° ê²€ì¦
            print(f"ğŸ” Step 6: ê²°ê³¼ í†µí•© ë° ê²€ì¦ ì¤‘...")
            trends, sections, citations = self._integrate_results(
                section_2_result, section_3_result, section_4_result, section_5_result
            )
            
            print(f"   íŠ¸ë Œë“œ: {len(trends)}ê°œ")
            print(f"   ì„¹ì…˜: {len(sections)}ê°œ")
            print(f"   ì¸ìš©: {len(citations)}ê°œ\n")
            
            # íŠ¸ë Œë“œ í‹°ì–´ë³„ ìš”ì•½
            hot_trends = [t for t in trends if t.is_hot_trend()]
            rising_stars = [t for t in trends if t.is_rising_star()]
            
            print(f"   ğŸ“ˆ HOT_TRENDS (1-2ë…„ ìƒìš©í™”): {len(hot_trends)}ê°œ")
            for trend in hot_trends[:3]:
                print(f"      - {trend.name} (ë…¼ë¬¸: {trend.paper_count}, ê¸°ì—… ë¹„ìœ¨: {trend.company_ratio:.2f})")
            
            print(f"\n   ğŸŒŸ RISING_STARS (3-5ë…„ í•µì‹¬ ê¸°ìˆ ): {len(rising_stars)}ê°œ")
            for trend in rising_stars[:3]:
                print(f"      - {trend.name} (ë…¼ë¬¸: {trend.paper_count}, ê¸°ì—… ë¹„ìœ¨: {trend.company_ratio:.2f})")
            
            # Citation ì¶œë ¥
            print(f"\n   ğŸ“š Citations (ì¶œì²˜):")
            arxiv_citations = [c for c in citations if c.source_type == "arxiv"]
            news_citations = [c for c in citations if c.source_type == "news"]
            report_citations = [c for c in citations if c.source_type == "report"]
            
            print(f"      - ArXiv ë…¼ë¬¸: {len(arxiv_citations)}ê°œ")
            for c in arxiv_citations[:3]:
                print(f"        [{c.number}] {c.title[:60]}...")
            
            print(f"      - ë‰´ìŠ¤ ê¸°ì‚¬: {len(news_citations)}ê°œ")
            for c in news_citations[:3]:
                print(f"        [{c.number}] {c.title[:60]}...")
            
            print(f"      - ì „ë¬¸ ë³´ê³ ì„œ: {len(report_citations)}ê°œ")
            for c in report_citations[:3]:
                print(f"        [{c.number}] {c.title[:60]}...")
            
            print(f"\n{'='*60}")
            print(f"âœ… Content Analysis ì™„ë£Œ!")
            print(f"{'='*60}\n")
            
            # State ì—…ë°ì´íŠ¸
            state["trends"] = trends
            state["sections"] = sections  # sections í‚¤ë¡œ ì €ì¥ (í‘œì¤€)
            state["citations"] = citations
            state["status"] = "analysis_complete"
            
            return state
        
        except Exception as e:
            print(f"âŒ Analysis ì‹¤íŒ¨: {e}")
            print(f"\nğŸ’¥ Content Analysis Agent ìµœì¢… ì‹¤íŒ¨\n")
            state["status"] = "analysis_failed"
            state["error"] = str(e)
            raise
    
    async def _run_parallel_sections(
        self,
        topic: str,
        keywords: List[str],
        data_summaries: Dict
    ) -> tuple:
        """Section 2, 3 ë³‘ë ¬ ì‹¤í–‰"""
        # Section 2 ì…ë ¥
        section_2_input = {
            "topic": topic,
            "keywords": ", ".join(keywords),
            "arxiv_count": len(data_summaries.get("arxiv_papers", [])),
            "arxiv_summary": data_summaries["arxiv"],
            "rag_summary": data_summaries["rag"]
        }
        
        # Section 3 ì…ë ¥
        section_3_input = {
            "topic": topic,
            "keywords": ", ".join(keywords),
            "trends_months": data_summaries.get("trends_months", 0),
            "trends_summary": data_summaries["trends"],
            "news_count": len(data_summaries.get("news_articles", [])),
            "news_summary": data_summaries["news"],
            "rag_summary": data_summaries["rag"],  # RAG summary ì¶”ê°€
            "citation_start_number": 100  # Section 2ì˜ ì¸ìš©ì´ ë¨¼ì €ì´ë¯€ë¡œ 100ë¶€í„° ì‹œì‘
        }
        
        # ë³‘ë ¬ ì‹¤í–‰
        section_2_task = self.section_2_chain.ainvoke(section_2_input)
        section_3_task = self.section_3_chain.ainvoke(section_3_input)
        
        section_2_result, section_3_result = await asyncio.gather(
            section_2_task, section_3_task
        )
        
        return section_2_result, section_3_result
    
    async def _run_section_4(
        self,
        topic: str,
        section_2_result: Dict,
        section_3_result: Dict,
        data_summaries: Dict
    ) -> Dict:
        """Section 4 ì‹¤í–‰ (Section 2, 3 ê¸°ë°˜)"""
        # Section 2, 3 ë‚´ìš© ì¶”ì¶œ
        section_2_content = "\n\n".join([
            f"**{key}:**\n{value}"
            for key, value in section_2_result.get("sections", {}).items()
        ])
        
        section_3_content = "\n\n".join([
            f"**{key}:**\n{value}"
            for key, value in section_3_result.get("sections", {}).items()
        ])
        
        # Trends ìš”ì•½
        trends_summary = "\n".join([
            f"- {trend['name']} ({trend['tier']}): {trend['reasoning'][:100]}..."
            for trend in section_2_result.get("trends", [])
        ])
        
        # ì¸ìš© ì‹œì‘ ë²ˆí˜¸ ê³„ì‚°
        citation_start = max([
            c.get("number", 0)
            for c in section_2_result.get("citations", []) + section_3_result.get("citations", [])
        ], default=0) + 1
        
        section_4_input = {
            "topic": topic,
            "section_2": section_2_content,  # í”„ë¡¬í”„íŠ¸ ë³€ìˆ˜ëª…ê³¼ ì¼ì¹˜
            "section_3": section_3_content,  # í”„ë¡¬í”„íŠ¸ ë³€ìˆ˜ëª…ê³¼ ì¼ì¹˜
            "rag_summary": data_summaries["rag"],
            "trends_summary": trends_summary,
            "citation_start_number": citation_start
        }
        
        section_4_result = await self.section_4_chain.ainvoke(section_4_input)
        
        return section_4_result
    
    async def _run_section_5(
        self,
        topic: str,
        section_2_result: Dict,
        section_3_result: Dict,
        section_4_result: Dict
    ) -> Dict:
        """Section 5 ì‹¤í–‰ (Section 2, 3, 4 ê¸°ë°˜)"""
        # ë‚´ìš© ì¶”ì¶œ
        section_2_content = "\n\n".join([
            f"**{key}:**\n{value}"
            for key, value in section_2_result.get("sections", {}).items()
        ])
        
        section_3_content = "\n\n".join([
            f"**{key}:**\n{value}"
            for key, value in section_3_result.get("sections", {}).items()
        ])
        
        section_4_content = "\n\n".join([
            f"**{key}:**\n{value}"
            for key, value in section_4_result.get("sections", {}).items()
        ])
        
        trends_summary = "\n".join([
            f"- {trend['name']} ({trend['tier']}): {trend['reasoning'][:100]}..."
            for trend in section_2_result.get("trends", [])
        ])
        
        # ì¸ìš© ì‹œì‘ ë²ˆí˜¸ ê³„ì‚°
        citation_start = max([
            c.get("number", 0)
            for c in (
                section_2_result.get("citations", []) +
                section_3_result.get("citations", []) +
                section_4_result.get("citations", [])
            )
        ], default=0) + 1
        
        section_5_input = {
            "topic": topic,
            "section_2": section_2_content,  # í”„ë¡¬í”„íŠ¸ ë³€ìˆ˜ëª…ê³¼ ì¼ì¹˜
            "section_3": section_3_content,  # í”„ë¡¬í”„íŠ¸ ë³€ìˆ˜ëª…ê³¼ ì¼ì¹˜
            "section_4": section_4_content,  # í”„ë¡¬í”„íŠ¸ ë³€ìˆ˜ëª…ê³¼ ì¼ì¹˜
            "trends_summary": trends_summary,
            "citation_start_number": citation_start
        }
        
        section_5_result = await self.section_5_chain.ainvoke(section_5_input)
        
        return section_5_result
    
    def _integrate_results(
        self,
        section_2_result: Dict,
        section_3_result: Dict,
        section_4_result: Dict,
        section_5_result: Dict
    ) -> tuple:
        """
        ëª¨ë“  ì„¹ì…˜ ê²°ê³¼ í†µí•©
        
        Returns:
            (trends, sections, citations)
        """
        # Trends ì¶”ì¶œ (Section 2ì—ì„œë§Œ)
        trends = []
        for trend_data in section_2_result.get("trends", []):
            trend = TrendTier(**trend_data)
            trends.append(trend)
        
        # Sections í†µí•©
        sections = {}
        sections.update(section_2_result.get("sections", {}))
        sections.update(section_3_result.get("sections", {}))
        sections.update(section_4_result.get("sections", {}))
        sections.update(section_5_result.get("sections", {}))
        
        # í•„ìˆ˜ ì„¹ì…˜ í™•ì¸
        required_sections = [
            "section_2_1",
            "section_2_2",
            "section_3_1",
            "section_3_2",
            "section_3_3",
            "section_4_1",
            "section_4_2",
            "section_5_1",
            "section_5_2",
            "section_5_3"
        ]
        
        for section in required_sections:
            if section not in sections:
                raise ValueError(f"í•„ìˆ˜ ì„¹ì…˜ ëˆ„ë½: {section}")
        
        # Citations í†µí•© ë° ì •ë ¬
        all_citations = (
            section_2_result.get("citations", []) +
            section_3_result.get("citations", []) +
            section_4_result.get("citations", []) +
            section_5_result.get("citations", [])
        )
        
        # ì¤‘ë³µ ì œê±° (number ê¸°ì¤€)
        seen_numbers = set()
        unique_citations = []
        for citation_data in all_citations:
            number = citation_data.get("number")
            if number not in seen_numbers:
                seen_numbers.add(number)
                
                # authorsê°€ ë¦¬ìŠ¤íŠ¸ë©´ ë¬¸ìì—´ë¡œ ë³€í™˜
                if isinstance(citation_data.get("authors"), list):
                    authors_list = citation_data.get("authors")
                    citation_data["authors"] = ", ".join(authors_list[:3])
                    if len(authors_list) > 3:
                        citation_data["authors"] += " et al."
                
                citation = CitationEntry(**citation_data)
                unique_citations.append(citation)
        
        # ë²ˆí˜¸ìˆœ ì •ë ¬
        unique_citations.sort(key=lambda c: c.number)
        
        return trends, sections, unique_citations
    
    def _create_data_summaries(
        self,
        arxiv_data: Dict,
        trends_data: Dict,
        news_data: Dict,
        rag_results: Dict
    ) -> Dict[str, Any]:
        """ë°ì´í„° ìš”ì•½ ìƒì„±"""
        summaries = {}
        
        # arXiv ìš”ì•½
        arxiv_papers = arxiv_data.get("papers", [])
        arxiv_summary = f"Total papers: {len(arxiv_papers)}\n\n"
        arxiv_summary += "Sample papers:\n"
        for i, paper in enumerate(arxiv_papers[:5], 1):
            arxiv_summary += f"{i}. {paper.get('title', 'N/A')}\n"
            arxiv_summary += f"   Authors: {', '.join(paper.get('authors', [])[:3])}\n"
            arxiv_summary += f"   Date: {paper.get('published', 'N/A')}\n"
            arxiv_summary += f"   Abstract: {paper.get('abstract', '')[:150]}...\n\n"
        summaries["arxiv"] = arxiv_summary
        summaries["arxiv_papers"] = arxiv_papers
        
        # Trends ìš”ì•½
        trends_summary = f"Total months: {trends_data.get('total_months', 0)}\n"
        trends_summary += f"Keywords tracked: {', '.join(trends_data.get('keywords', []))}\n\n"
        trends_summary += "Sample data points:\n"
        for i, data_point in enumerate(trends_data.get('data', [])[:3], 1):
            trends_summary += f"{i}. Date: {data_point.get('date', 'N/A')}\n"
            for key, value in data_point.items():
                if key != 'date':
                    trends_summary += f"   {key}: {value}\n"
            trends_summary += "\n"
        summaries["trends"] = trends_summary
        summaries["trends_months"] = trends_data.get('total_months', 0)
        
        # News ìš”ì•½
        news_articles = news_data.get("articles", [])
        news_summary = f"Total articles: {len(news_articles)}\n"
        news_summary += f"Unique sources: {news_data.get('unique_sources', 0)}\n\n"
        news_summary += "Sample articles:\n"
        for i, article in enumerate(news_articles[:5], 1):
            news_summary += f"{i}. {article.get('title', 'N/A')}\n"
            news_summary += f"   Source: {article.get('source', 'N/A')}\n"
            news_summary += f"   Date: {article.get('published', 'N/A')}\n"
            news_summary += f"   Snippet: {article.get('snippet', '')[:100]}...\n\n"
        summaries["news"] = news_summary
        summaries["news_articles"] = news_articles
        
        # RAG ìš”ì•½
        rag_summary = f"Total results: {rag_results.get('total_results', 0)}\n\n"
        rag_summary += "Key insights from reference documents:\n"
        for i, result in enumerate(rag_results.get('results', [])[:5], 1):
            rag_summary += f"{i}. Source: {result.get('source', 'N/A')} (Page {result.get('page', 'N/A')})\n"
            rag_summary += f"   Content: {result.get('content', '')[:200]}...\n\n"
        summaries["rag"] = rag_summary
        
        return summaries