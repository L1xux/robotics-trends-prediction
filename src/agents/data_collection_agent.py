"""
Data Collection Agent (ReAct Architecture)

ë°ì´í„° ìˆ˜ì§‘ì„ ë‹´ë‹¹í•˜ëŠ” Agent
- ArXiv ë…¼ë¬¸ ìˆ˜ì§‘ ë° í‚¤ì›Œë“œ ì¶”ì¶œ (ë¨¼ì € ì‹¤í–‰)
- RAG + Newsë¥¼ ReAct Agentê°€ ììœ¨ì ìœ¼ë¡œ ì‚¬ìš©
- ë°ì´í„° ì¶©ë¶„ì„± íŒë‹¨ (ë³´ê³ ì„œ ì‘ì„± ê°€ëŠ¥ ì—¬ë¶€)
- Citation ì •ë³´ ìˆ˜ì§‘
"""

import json
import time
from typing import List, Any, Dict, Optional
from datetime import datetime

from langchain_core.language_models import BaseChatModel
from langchain_openai import ChatOpenAI
from langchain_classic.agents import AgentExecutor, create_react_agent
from langchain_core.prompts import PromptTemplate

from src.agents.base.base_agent import BaseAgent
from src.agents.base.agent_config import AgentConfig
from src.graph.state import PipelineState
from src.core.settings import Settings
from src.core.models.citation_model import (
    ArXivCitation, NewsCitation, RAGCitation, CitationCollection
)
from config.prompts.data_collections_prompts import (
    REACT_SYSTEM_PROMPT,
    SUFFICIENCY_CHECK_PROMPT
)


class DataCollectionAgent(BaseAgent):
    """
    Data Collection Agent (ReAct Architecture)
    
    Workflow:
    1. ArXiv ë…¼ë¬¸ ìˆ˜ì§‘ (ë¨¼ì € ì‹¤í–‰, tool ì•„ë‹˜)
    2. ë…¼ë¬¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
    3. ReAct Agentê°€ RAG + News toolì„ ììœ¨ì ìœ¼ë¡œ ì‚¬ìš©
       - í™•ì¥ëœ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰
       - ì¶©ë¶„í•  ë•Œê¹Œì§€ ë°˜ë³µ
    4. ë°ì´í„° ì¶©ë¶„ì„± íŒë‹¨ (ë³´ê³ ì„œ ì‘ì„± ê°€ëŠ¥ ì—¬ë¶€)
    5. ë¶€ì¡±í•˜ë©´ Agentê°€ ì¶”ê°€ ìˆ˜ì§‘
    
    Features:
    - ArXiv: ì´ˆê¸° ì‹¤í–‰ìœ¼ë¡œ ê¸°ìˆ  landscape íŒŒì•…
    - ReAct Agent: RAG + News toolë§Œ ì‚¬ìš©
    - Citation ìë™ ìƒì„±
    - ì¶©ë¶„ì„± íŒë‹¨: ë³´ê³ ì„œ ëª©ì°¨ ê¸°ì¤€
    """
    
    def __init__(
        self,
        llm: BaseChatModel,
        tools: List[Any],
        config: AgentConfig,
        raw_tools: Optional[List[Any]] = None,
        settings: Optional[Settings] = None
    ):
        super().__init__(llm, tools, config)
        self.settings = settings or Settings()
        
        # Raw tools for direct access
        self.raw_tools = raw_tools or []
        self.arxiv_tool = self.raw_tools[0] if len(self.raw_tools) > 0 else None
        self.rag_tool = self.raw_tools[1] if len(self.raw_tools) > 1 else None
        self.news_tool = self.raw_tools[2] if len(self.raw_tools) > 2 else None
        
        # ì¶©ë¶„ì„± íŒë‹¨ìš© LLM (GPT-4o)
        self.sufficiency_llm = ChatOpenAI(
            model="gpt-4o",
            temperature=0.3
        )
        
        # ReAct Agent ì„¤ì • (RAG + Newsë§Œ ì‚¬ìš©)
        self._setup_react_agent()
    
    def _setup_react_agent(self):
        """ReAct Agent ì„¤ì • (RAG + News toolë§Œ)"""
        # toolsëŠ” ì´ë¯¸ RAG + Newsë§Œ í¬í•¨ (workflow.pyì—ì„œ ì„¤ì •)
        
        # ReAct Prompt Template
        react_template = """You are a data collection specialist for AI-Robotics trend reports.

You have already collected ArXiv papers and extracted technical keywords.
Now use these tools to gather market trends and expert forecasts:

{tools}

STRICT FORMAT:

Question: the input question you must answer
Thought: you should always think about what to do next
Action: the action to take, should be one of [{tool_names}]
Action Input: the input to the action (valid JSON)
Observation: the result of the action
... (repeat Thought/Action/Action Input/Observation as needed)
Thought: I have collected sufficient data
Final Answer: summary of collected data

Guidelines:
1. Use search_reference_documents for expert forecasts and technology trends
2. Use search_tech_news for company activities and real-world applications
3. Try different keyword combinations to get diverse sources
4. Collect at least 10 RAG documents and 20 news articles
5. When you have enough data, provide Final Answer

Begin!

Question: {input}
Thought:{agent_scratchpad}"""
        
        react_prompt = PromptTemplate(
            template=react_template,
            input_variables=["input", "tools", "tool_names", "agent_scratchpad"]
        )
        
        # ReAct Agent ìƒì„± (RAG + Newsë§Œ)
        self.react_agent = create_react_agent(
            llm=self.llm,
            tools=self.tools,  # RAG + News wrappers
            prompt=react_prompt
        )
        
        # Agent Executor
        self.agent_executor = AgentExecutor(
            agent=self.react_agent,
            tools=self.tools,  # RAG + News wrappers
            verbose=True,
            max_iterations=25,
            max_execution_time=1200,  # 20ë¶„
            handle_parsing_errors=True,
            return_intermediate_steps=True
        )
    
    async def execute(self, state: PipelineState) -> PipelineState:
        """
        Data Collection ì‹¤í–‰
        
        Workflow:
        1. ArXiv ë…¼ë¬¸ ìˆ˜ì§‘ (ì§ì ‘ ì‹¤í–‰)
        2. í‚¤ì›Œë“œ í™•ì¥
        3. ReAct Agentë¡œ RAG + News ìˆ˜ì§‘
        4. ì¶©ë¶„ì„± íŒë‹¨
        5. ë¶€ì¡±í•˜ë©´ ì¬ì‹œë„
        """
        print(f"\n{'='*60}")
        print(f"ğŸ“š Data Collection Agent ì‹¤í–‰ ì¤‘...")
        print(f"{'='*60}\n")
        
        # Stateì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        planning_output = state.get("planning_output")
        if not planning_output:
            raise ValueError("planning_outputì´ Stateì— ì—†ìŠµë‹ˆë‹¤.")
        
        topic = planning_output.topic
        keywords = state.get("keywords", [])
        
        print(f"ğŸ“ ì£¼ì œ: {topic}")
        print(f"ğŸ”‘ ì´ˆê¸° í‚¤ì›Œë“œ: {', '.join(keywords)}\n")
        
        # ìµœëŒ€ ì¬ì‹œë„
        max_attempts = 3
        attempt = 0
        
        # ìˆ˜ì§‘ ê²°ê³¼
        arxiv_data = None
        rag_results = None
        news_data = None
        expanded_keywords = []
        citations = CitationCollection()
        
        while attempt < max_attempts:
            attempt += 1
            print(f"\n{'='*60}")
            print(f"ğŸ”„ ë°ì´í„° ìˆ˜ì§‘ ì‹œë„ {attempt}/{max_attempts}")
            print(f"{'='*60}\n")
            
            try:
                # ========================================
                # Step 1: ArXiv ë…¼ë¬¸ ìˆ˜ì§‘ (ë¨¼ì € ì‹¤í–‰)
                # ========================================
                if attempt == 1:  # ì²« ì‹œë„ì—ë§Œ ArXiv ìˆ˜ì§‘
                    print(f"ğŸ“„ Step 1: ArXiv ë…¼ë¬¸ ìˆ˜ì§‘ ì¤‘...")
                    arxiv_data = await self._collect_arxiv(keywords, planning_output)
                    
                    if arxiv_data and arxiv_data.get("total_count", 0) > 0:
                        print(f"   âœ… ArXiv ìˆ˜ì§‘ ì™„ë£Œ: {arxiv_data['total_count']}í¸")
                        # Toolì—ì„œ ìƒì„±í•œ citations ê°€ì ¸ì˜¤ê¸°
                        tool_citations = arxiv_data.get("citations", [])
                        citations.arxiv_citations.extend(tool_citations)
                        print(f"   âœ… ArXiv Citation: {len(tool_citations)}ê°œ\n")
                    else:
                        print(f"   âš ï¸  ArXiv ë°ì´í„° ì—†ìŒ\n")
                    
                    # Step 2: í‚¤ì›Œë“œ ì¶”ì¶œ
                    print(f"ğŸ”‘ Step 2: ë…¼ë¬¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...")
                    expanded_keywords = await self._expand_keywords(arxiv_data, keywords)
                    print(f"   âœ… ì¶”ì¶œëœ í‚¤ì›Œë“œ ({len(expanded_keywords)}ê°œ): {', '.join(expanded_keywords[:10])}{'...' if len(expanded_keywords) > 10 else ''}\n")
                
                # ========================================
                # Step 3: ReAct Agentë¡œ RAG + News ìˆ˜ì§‘
                # ========================================
                print(f"ğŸ¤– Step 3: ReAct Agent ì‹œì‘ (RAG + News)...\n")
                
                # Agentì—ê²Œ ì¤„ ì§ˆë¬¸ ìƒì„±
                question = self._generate_agent_question(
                    topic, expanded_keywords, arxiv_data, attempt
                )
                
                # ReAct Agent ì‹¤í–‰
                result = await self.agent_executor.ainvoke({
                    "input": question
                })
                
                print(f"\nâœ… ReAct Agent ì™„ë£Œ!\n")
                
                # Wrapper ìºì‹œì—ì„œ Agentê°€ ìˆ˜ì§‘í•œ ë°ì´í„° ì¶”ì¶œ (ì¬ê²€ìƒ‰ ì—†ìŒ!)
                print(f"ğŸ“Š Agent ìˆ˜ì§‘ ë°ì´í„° ì¶”ì¶œ ì¤‘ (ìºì‹œ ì‚¬ìš©)...\n")
                
                rag_results, news_data = self._extract_data_from_cache(
                    topic, expanded_keywords
                )
                
                # Citation ì¶”ê°€ (Toolì—ì„œ ìƒì„±í•œ ê²ƒ ê°€ì ¸ì˜¤ê¸°)
                if rag_results:
                    tool_citations = rag_results.get("citations", [])
                    citations.rag_citations.extend(tool_citations)
                    print(f"   âœ… RAG Citation: {len(tool_citations)}ê°œ")
                
                if news_data:
                    tool_citations = news_data.get("citations", [])
                    citations.news_citations.extend(tool_citations)
                    print(f"   âœ… News Citation: {len(tool_citations)}ê°œ\n")
                
                # ========================================
                # Step 4: ë°ì´í„° ì¶©ë¶„ì„± íŒë‹¨
                # ========================================
                print(f"ğŸ” Step 4: ë°ì´í„° ì¶©ë¶„ì„± íŒë‹¨ ì¤‘...")
                sufficiency_result = await self._check_sufficiency(
                    topic, keywords, expanded_keywords,
                    arxiv_data, rag_results, news_data
                )
                
                print(f"   ğŸ“Š ì¶©ë¶„ì„± ì ìˆ˜: {sufficiency_result.get('overall_score', 0):.2f}")
                print(f"   íŒì •: {'âœ… ì¶©ë¶„í•¨' if sufficiency_result.get('sufficient', False) else 'âš ï¸  ë¶€ì¡±í•¨'}\n")
                
                # ì¶©ë¶„í•˜ë©´ ì¢…ë£Œ
                if sufficiency_result.get("sufficient", False):
                    print(f"âœ… ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ! (ë³´ê³ ì„œ ì‘ì„± ê°€ëŠ¥)\n")
                    break
                
                # ë¶€ì¡±í•˜ë©´ ì¬ì‹œë„
                if attempt < max_attempts:
                    print(f"âš ï¸  ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ì¶”ê°€ ìˆ˜ì§‘ì„ ì‹œë„í•©ë‹ˆë‹¤.")
                    print(f"   ë¶€ì¡±í•œ ì˜ì—­: {', '.join(sufficiency_result.get('missing_areas', []))}")
                    print(f"   ê¶Œì¥ì‚¬í•­: {', '.join(sufficiency_result.get('recommendations', []))}\n")
                    
                    # í‚¤ì›Œë“œ ì¶”ê°€ í™•ì¥
                    expanded_keywords = self._further_expand_keywords(expanded_keywords)
                else:
                    print(f"âš ï¸  ìµœëŒ€ ì‹œë„ íšŸìˆ˜ ë„ë‹¬. í˜„ì¬ ë°ì´í„°ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.\n")
            
            except Exception as e:
                print(f"âŒ ë°ì´í„° ìˆ˜ì§‘ ì¤‘ ì—ëŸ¬: {e}")
                print(f"error")
                
                if attempt >= max_attempts:
                    print(f"\nâš ï¸  ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ë„ë‹¬. ìˆ˜ì§‘ëœ ë°ì´í„°ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.\n")
                    break
                
                print(f"ğŸ”„ ì¬ì‹œë„ ì¤‘...\n")
                time.sleep(2)
                continue
        
        # ê²°ê³¼ ìš”ì•½
        print(f"\n{'='*60}")
        print(f"ğŸ“Š ë°ì´í„° ìˆ˜ì§‘ ê²°ê³¼ ìš”ì•½")
        print(f"{'='*60}")
        print(f"ğŸ“„ ArXiv ë…¼ë¬¸: {arxiv_data.get('total_count', 0) if arxiv_data else 0}í¸")
        print(f"ğŸ“– RAG ê²°ê³¼: {rag_results.get('total_results', 0) if rag_results else 0}ê°œ")
        print(f"ğŸ“° ë‰´ìŠ¤ ê¸°ì‚¬: {news_data.get('total_articles', 0) if news_data else 0}ê°œ")
        print(f"ğŸ”‘ ì¶”ì¶œ í‚¤ì›Œë“œ: {len(expanded_keywords)}ê°œ")
        print(f"ğŸ“š ì¸ìš© ì •ë³´ (ì¶œì²˜):")
        print(f"   - ArXiv ë…¼ë¬¸: {len(citations.arxiv_citations)}ê°œ")
        print(f"   - ë‰´ìŠ¤ ê¸°ì‚¬: {len(citations.news_citations)}ê°œ")
        print(f"   - RAG ë¬¸ì„œ: {len(citations.rag_citations)}ê°œ")
        print(f"   - ì´ {len(citations.get_all_citations())}ê°œ ì¶œì²˜")
        print(f"{'='*60}\n")
        
        # State ì—…ë°ì´íŠ¸
        state["arxiv_data"] = arxiv_data or {}
        state["news_data"] = news_data or {}
        state["rag_results"] = rag_results or {}
        state["expanded_keywords"] = expanded_keywords
        state["citations"] = citations
        state["status"] = "data_collection_complete"
        
        return state
    
    def _extract_data_from_cache(
        self,
        topic: str,
        expanded_keywords: List[str]
    ) -> tuple:
        """
        Wrapper ìºì‹œì—ì„œ Agentê°€ ìˆ˜ì§‘í•œ ë°ì´í„° ì¶”ì¶œ (ì¬ê²€ìƒ‰ ë¶ˆí•„ìš”!)
        
        Returns:
            (rag_results, news_data)
        """
        # RAG wrapperì—ì„œ ìºì‹œ ê°€ì ¸ì˜¤ê¸°
        rag_wrapper = None
        news_wrapper = None
        
        for tool in self.tools:
            if 'reference' in tool.name.lower() or 'document' in tool.name.lower():
                rag_wrapper = tool
            elif 'news' in tool.name.lower():
                news_wrapper = tool
        
        # RAG ìºì‹œ ë°ì´í„° í†µí•©
        rag_documents = []
        rag_citations = []
        seen_contents = set()
        seen_rag_citations = set()  # Citation ì¤‘ë³µ ì œê±°ìš©
        
        if rag_wrapper and hasattr(rag_wrapper, 'get_cached_data'):
            cached_rag = rag_wrapper.get_cached_data()
            print(f"   ğŸ“– RAG ìºì‹œ: {len(cached_rag)}ë²ˆ ê²€ìƒ‰ ìˆ˜í–‰ë¨")
            
            for cache_entry in cached_rag:
                entry_docs = cache_entry.get("documents", [])
                entry_citations = cache_entry.get("citations", [])
                
                # Documentì™€ Citationì„ í•¨ê»˜ ì²˜ë¦¬ (1:1 ë§¤ì¹­ ê°€ì •)
                for i, doc in enumerate(entry_docs):
                    content = doc.get("content", "")
                    if content and content not in seen_contents:
                        seen_contents.add(content)
                        rag_documents.append(doc)
                        
                        # ëŒ€ì‘í•˜ëŠ” citation ì¶”ê°€ (ì¤‘ë³µ ì²´í¬)
                        if i < len(entry_citations):
                            citation = entry_citations[i]
                            citation_key = citation.full_citation if hasattr(citation, 'full_citation') else str(citation)
                            if citation_key not in seen_rag_citations:
                                seen_rag_citations.add(citation_key)
                                rag_citations.append(citation)
        
        # News ìºì‹œ ë°ì´í„° í†µí•©
        news_articles = []
        news_citations = []
        seen_urls = set()
        seen_news_citations = set()  # Citation ì¤‘ë³µ ì œê±°ìš©
        
        if news_wrapper and hasattr(news_wrapper, 'get_cached_data'):
            cached_news = news_wrapper.get_cached_data()
            print(f"   ğŸ“° News ìºì‹œ: {len(cached_news)}ë²ˆ ê²€ìƒ‰ ìˆ˜í–‰ë¨")
            
            for cache_entry in cached_news:
                entry_articles = cache_entry.get("articles", [])
                entry_citations = cache_entry.get("citations", [])
                
                # Articleê³¼ Citationì„ í•¨ê»˜ ì²˜ë¦¬ (1:1 ë§¤ì¹­ ê°€ì •)
                for i, article in enumerate(entry_articles):
                    url = article.get("url", "")
                    if url and url not in seen_urls:
                        seen_urls.add(url)
                        news_articles.append(article)
                        
                        # ëŒ€ì‘í•˜ëŠ” citation ì¶”ê°€ (ì¤‘ë³µ ì²´í¬)
                        if i < len(entry_citations):
                            citation = entry_citations[i]
                            citation_key = citation.full_citation if hasattr(citation, 'full_citation') else str(citation)
                            if citation_key not in seen_news_citations:
                                seen_news_citations.add(citation_key)
                                news_citations.append(citation)
        
        # RAG ê²°ê³¼ ì •ë¦¬
        rag_results = {
            "query": topic,
            "search_type": "agent_collected_cached",
            "total_results": len(rag_documents),
            "documents": rag_documents,
            "citations": rag_citations  # Citations ì¶”ê°€
        } if rag_documents else None
        
        # News ê²°ê³¼ ì •ë¦¬
        news_data = {
            "keywords": expanded_keywords,
            "date_range": "3 years",
            "total_articles": len(news_articles),
            "unique_sources": len(set(a.get("source", "") for a in news_articles)),
            "articles": news_articles,
            "citations": news_citations  # Citations ì¶”ê°€
        } if news_articles else None
        
        # ìµœì¢… í†µê³„ ì¶œë ¥
        if rag_results:
            print(f"   âœ… RAG ë°ì´í„°: {len(rag_documents)}ê°œ ë¬¸ì„œ ì¶”ì¶œë¨")
        if news_data:
            print(f"   âœ… News ë°ì´í„°: {len(news_articles)}ê°œ ê¸°ì‚¬ ì¶”ì¶œë¨\n")
        
        return rag_results, news_data
    
    def _generate_agent_question(
        self,
        topic: str,
        keywords: List[str],
        arxiv_data: Optional[Dict],
        attempt: int
    ) -> str:
        """ReAct Agentì—ê²Œ ì¤„ ì§ˆë¬¸ ìƒì„±"""
        paper_count = arxiv_data.get("total_count", 0) if arxiv_data else 0
        
        if attempt == 1:
            # ë¡œë´‡/ê¸°ìˆ  ê´€ë ¨ í‚¤ì›Œë“œë§Œ ì„ ë³„
            question = f"""Collect comprehensive data for a ROBOTICS/AUTOMATION technology trend report on "{topic}".

I have already collected {paper_count} ArXiv papers and extracted these ROBOTICS-SPECIFIC keywords:
{', '.join(keywords[:])}

Your task:
1. Use search_reference_documents to find expert forecasts about ROBOTICS/AI technologies
   - Search for: robotics trends, AI in automation, 5-year predictions for robots
   - Look for: industrial robotics, service robots, autonomous systems
   - Get: future technology forecasts, market adoption predictions
   - Use TECHNOLOGY keywords from the list above
   - Target: At least 10 relevant documents

2. Use search_tech_news to find recent ROBOTICS/AI news
   - Use ONLY technology keywords (e.g., "collaborative robots", "adaptive welding")
   - Focus on: robot deployments, new robot products, automation innovations
   - Search for: specific companies + technology combinations
   - DO NOT use generic terms like "market activities", "announcements"
   - Target: At least 20 diverse articles about robot technologies

**IMPORTANT:** Use SPECIFIC technology keywords, not generic business terms!
When you have enough data, provide a final summary."""
        
        else:
            question = f"""Continue collecting more data for "{topic}".

Current status: Need more diverse sources.

Use the keywords: {', '.join(keywords[:10])}

Focus on:
- Finding different sources and perspectives
- Covering various aspects of the technology
- Getting recent market activities

Continue until you have comprehensive coverage."""
        
        return question
    
    async def _collect_arxiv(
        self,
        keywords: List[str],
        planning_output: Any
    ) -> Optional[Dict[str, Any]]:
        """ArXiv ë…¼ë¬¸ ìˆ˜ì§‘ (í‚¤ì›Œë“œë³„ ë³‘ë ¬ ê²€ìƒ‰)"""
        max_retries = 3
        
        for retry in range(max_retries):
            try:
                if not self.arxiv_tool:
                    print("   âš ï¸  ArXiv Toolì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    return None
                
                # collection_plan ì‚¬ìš©
                categories_str = planning_output.collection_plan.arxiv.categories
                
                # categoriesê°€ "all"ì´ë©´ ê¸°ë³¸ ì¹´í…Œê³ ë¦¬ ì‚¬ìš©
                if categories_str.lower() == "all":
                    categories_str = "cs.RO,cs.AI"
                
                print(f"   ğŸ“š ì¹´í…Œê³ ë¦¬: {categories_str}")
                print(f"   ğŸ”‘ í‚¤ì›Œë“œ: {len(keywords)}ê°œ (ê° í‚¤ì›Œë“œë‹¹ ë³‘ë ¬ ê²€ìƒ‰)")
                print(f"   ğŸ¯ í‚¤ì›Œë“œë‹¹ ìµœëŒ€: 100í¸\n")
                
                # í‚¤ì›Œë“œë³„ ë³‘ë ¬ ê²€ìƒ‰ (ìµœê·¼ 3ë…„)
                result = self.arxiv_tool.search_by_keywords_parallel(
                    keywords=keywords,
                    categories=categories_str,
                    max_results_per_keyword=100,
                    years_back=3
                )
                
                # ì„±ê³µí•˜ë©´ ë°˜í™˜
                if result and result.get("total_count", 0) > 0:
                    return result
                else:
                    print(f"   âš ï¸  ë…¼ë¬¸ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì¬ì‹œë„ {retry + 1}/{max_retries}")
                    if retry < max_retries - 1:
                        time.sleep(3)
                        continue
                    return result
            
            except Exception as e:
                print(f"   âŒ ArXiv ìˆ˜ì§‘ ì—ëŸ¬ (ì‹œë„ {retry + 1}/{max_retries}): {e}")
                if retry < max_retries - 1:
                    print(f"   ğŸ”„ 3ì´ˆ í›„ ì¬ì‹œë„...")
                    time.sleep(3)
                    continue
                else:
                    print(f"   error")
                    return None
        
        return None
    
    
    async def _expand_keywords(
        self,
        arxiv_data: Optional[Dict[str, Any]],
        initial_keywords: List[str]
    ) -> List[str]:
        """
        ArXiv ë…¼ë¬¸ì—ì„œ emerging/íŠ¹ì´í•œ ê¸°ìˆ  í‚¤ì›Œë“œ ì¶”ì¶œ
        
        ëª©í‘œ: 5ë…„ í›„ íŠ¸ë Œë“œë¥¼ ì˜ˆì¸¡í•  ìˆ˜ ìˆëŠ” ìƒˆë¡­ê³  êµ¬ì²´ì ì¸ í‚¤ì›Œë“œ ë°œêµ´
        
        1. ë…¼ë¬¸ ì œëª©/ì´ˆë¡ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
        2. ê¸°ì—… ì •ë³´ ì¶”ì¶œ
        3. LLMì´ emerging/specific í‚¤ì›Œë“œ ì„ ë³„
           - ì¼ë°˜ì  í‚¤ì›Œë“œ(machine learning) ì œì™¸
           - êµ¬ì²´ì /ìƒˆë¡œìš´ í‚¤ì›Œë“œ(neuromorphic computing) ìš°ì„ 
        """
        if not arxiv_data or not arxiv_data.get("papers"):
            return initial_keywords  # ë…¼ë¬¸ì´ ì—†ìœ¼ë©´ ì´ˆê¸° í‚¤ì›Œë“œ ì‚¬ìš©
        
        # 1. ë…¼ë¬¸ì—ì„œ ì¶”ì¶œëœ í‚¤ì›Œë“œ ìˆ˜ì§‘
        raw_keywords = set()
        for paper in arxiv_data["papers"]:
            paper_keywords = paper.get("keywords", [])
            for kw in paper_keywords:
                raw_keywords.add(kw)
        
        # 2. ë…¼ë¬¸ì—ì„œ ì–¸ê¸‰ëœ ê¸°ì—… ì¶”ì¶œ
        companies_mentioned = set()
        if "companies_mentioned" in arxiv_data:
            for company, count in arxiv_data["companies_mentioned"].items():
                if count >= 2:  # 2ë²ˆ ì´ìƒ ì–¸ê¸‰ëœ ê¸°ì—…ë§Œ
                    companies_mentioned.add(company)
        
        # 3. ë…¼ë¬¸ ì œëª© ë¶„ì„ (ìµœì‹  ê¸°ìˆ  ê²½í–¥ íŒŒì•…)
        recent_papers_info = []
        if arxiv_data.get("papers"):
            # ìµœì‹  ë…¼ë¬¸ 20ê°œ ì •ë„ë§Œ ë¶„ì„ (ìµœì‹  íŠ¸ë Œë“œ)
            for paper in arxiv_data["papers"][:20]:
                recent_papers_info.append({
                    "title": paper.get("title", ""),
                    "year": paper.get("published", "")[:4]  # YYYYë§Œ ì¶”ì¶œ
                })
        
        # 4. ëª¨ë“  í›„ë³´ í‚¤ì›Œë“œ ê²°í•©
        all_candidates = list(raw_keywords) + list(companies_mentioned)
        
        # ë…¼ë¬¸ í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ ì´ˆê¸° í‚¤ì›Œë“œ ì‚¬ìš©
        if not all_candidates:
            return initial_keywords
        
        # 5. LLMìœ¼ë¡œ emerging/specific í‚¤ì›Œë“œ ì„ ë³„ â­
        filtered_keywords = await self._filter_emerging_keywords(
            initial_keywords=initial_keywords,
            raw_keywords=all_candidates,
            companies=list(companies_mentioned),
            recent_papers=recent_papers_info
        )
        
        return filtered_keywords
    
    async def _filter_emerging_keywords(
        self,
        initial_keywords: List[str],
        raw_keywords: List[str],
        companies: Optional[List[str]] = None,
        recent_papers: Optional[List[Dict]] = None
    ) -> List[str]:
        """
        LLMì„ ì‚¬ìš©í•˜ì—¬ emerging/specific í‚¤ì›Œë“œ ì„ ë³„
        
        ëª©í‘œ: 5ë…„ í›„ íŠ¸ë Œë“œ ì˜ˆì¸¡ì„ ìœ„í•œ ìƒˆë¡­ê³  êµ¬ì²´ì ì¸ í‚¤ì›Œë“œ ë°œêµ´
        
        Args:
            initial_keywords: ì‚¬ìš©ì ì§ˆë¬¸ ê¸°ë°˜ ì´ˆê¸° í‚¤ì›Œë“œ
            raw_keywords: arXiv ë…¼ë¬¸ì—ì„œ ì¶”ì¶œëœ ëª¨ë“  í‚¤ì›Œë“œ
            companies: ë…¼ë¬¸ì—ì„œ ì–¸ê¸‰ëœ ê¸°ì—… ë¦¬ìŠ¤íŠ¸
            recent_papers: ìµœì‹  ë…¼ë¬¸ ì œëª© ë¦¬ìŠ¤íŠ¸
        
        Returns:
            emerging/specific í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ (ê¸°ì—… í¬í•¨)
        """
        companies = companies or []
        recent_papers = recent_papers or []
        
        from langchain_core.prompts import ChatPromptTemplate
        from langchain_core.output_parsers import StrOutputParser
        
        # ìµœì‹  ë…¼ë¬¸ ì œëª© ìš”ì•½
        paper_titles = "\n".join([f"- ({p['year']}) {p['title']}" for p in recent_papers[:15]])
        
        filter_prompt = ChatPromptTemplate.from_messages([
            ("system", """You are an expert at identifying EMERGING and SPECIFIC technology trends for 5-year forecasting.

**Your Mission: Find keywords that predict the FUTURE, not describe the PRESENT**

**PRIORITY 1 - EMERGING Technologies (SELECT THESE!):**
- âœ… New/novel technical approaches appearing in recent papers
- âœ… Specific method names (e.g., "neuromorphic computing", "liquid neural networks")
- âœ… Emerging application areas (e.g., "soft robotics", "bio-inspired actuation")
- âœ… Next-generation concepts (e.g., "edge AI", "federated learning")
- âœ… Interdisciplinary technologies (e.g., "human-robot collaboration", "explainable robotics")
- âœ… Specific hardware innovations (e.g., "tactile sensors", "compliant actuators")

**PRIORITY 2 - Companies (ALWAYS KEEP):**
- âœ… ALL company names indicate WHO is investing in the future
- âœ… Companies show market validation of technologies

**REJECT - Generic/Obvious Keywords (FILTER OUT!):**
- âŒ Generic terms: "machine learning", "deep learning", "neural networks"
- âŒ Obvious concepts: "automation", "robotics", "AI"
- âŒ Too broad: "manufacturing", "industry", "production"
- âŒ Implementation details: version numbers, dataset names, model sizes
- âŒ Programming tools: languages, frameworks

**Strategy for 5-Year Trend Prediction:**
1. Look for SPECIFIC technologies that are NEW in recent papers
2. Identify CONCRETE technical methods, not broad categories
3. Find technologies that combine multiple fields (interdisciplinary)
4. Select keywords that will help find DETAILED expert reports and news

**Output:** JSON list of 25-35 keywords (20-25 emerging tech + companies)
Format: ["specific_tech1", "emerging_method2", "company1", ...]

Remember: We can find "machine learning" anywhere. We need SPECIFIC technologies like "sim-to-real transfer" or "tactile manipulation"!"""),
            ("user", """**User's Original Query:**
{initial_keywords}

**Recent Paper Titles (Latest Research Trends):**
{paper_titles}

**Raw Keywords Extracted from Papers:**
{raw_keywords}

**Companies Mentioned in Papers:**
{companies}

**Your Task:**
Analyze the recent paper titles and keywords to identify **ROBOTICS/AUTOMATION-RELATED** technologies.

**Context:** User query is "{initial_keywords}" - focus on ROBOTICS and AI technologies broadly.

Selection criteria:
1. Technology MUST relate to **robotics, AI, or automation** (any application domain)
2. EMERGING/SPECIFIC technologies that are NEW or NOVEL  
3. Technologies appearing repeatedly in recent papers (trending up)
4. Include: robot hardware, algorithms, control, perception, applications
5. ALL companies (they show market activity)

Filter and return 25-35 keywords for finding future robotics trends:

**âœ… KEEP - Robotics Technologies:**
- Robot hardware (actuators, sensors, mechanisms, grippers)
- Robot AI/learning (imitation learning, sim-to-real, RL for robots)
- Robot perception (3D vision, tactile, depth estimation)
- Robot control (force control, compliance, motion planning)
- Robot applications (manufacturing, surgery, service, warehouse, etc.)
- Specific tech terms (adaptive welding, bin-picking, collaborative robots)

**âŒ REJECT - Non-Robotics:**
- Pure ML/statistics without robotics (Bayesian optimization, clustering)
- General software (JSON, API, databases)
- Business jargon (market growth, ROI, stakeholders)
- Version numbers (GPT-4, v2.0, Python 3)

**Note:** "Differential Mechanism" = robot hardware âœ…
"Gaussian Splats" = 3D perception for robots âœ…  
"Multi-Agent Learning" = robot coordination âœ…
Keep if it can be used BY or FOR robots!""")
        ])
        
        try:
            chain = filter_prompt | self.llm | StrOutputParser()
            response = await chain.ainvoke({
                "initial_keywords": ", ".join(initial_keywords),
                "paper_titles": paper_titles if paper_titles else "No recent papers available",
                "raw_keywords": ", ".join(raw_keywords[:100]),  # ë„ˆë¬´ ë§ìœ¼ë©´ ì²˜ìŒ 100ê°œë§Œ
                "companies": ", ".join(companies) if companies else "None mentioned"
            })
            
            # JSON íŒŒì‹±
            import re
            json_match = re.search(r'\[.*?\]', response, re.DOTALL)
            if json_match:
                filtered = json.loads(json_match.group(0))
                # ì´ˆê¸° í‚¤ì›Œë“œëŠ” í•­ìƒ í¬í•¨
                final_keywords = list(set(initial_keywords + filtered))
                
                # í†µê³„ ì¶œë ¥
                tech_keywords = [kw for kw in filtered if kw not in companies]
                company_keywords = [kw for kw in filtered if kw in companies]
                
                print(f"   ğŸ” Emerging keyword extraction:")
                print(f"      â€¢ Raw candidates: {len(raw_keywords)}")
                print(f"      â€¢ Emerging/specific tech: {len(tech_keywords)}")
                print(f"      â€¢ Companies identified: {len(company_keywords)}")
                print(f"      â€¢ Total (with initial): {len(final_keywords)}")
                print(f"      â€¢ Top emerging tech: {', '.join(tech_keywords[:5])}...")
                
                if company_keywords:
                    print(f"      â€¢ Companies: {', '.join(company_keywords[:5])}{'...' if len(company_keywords) > 5 else ''}")
                
                return sorted(final_keywords[:40])  # ìµœëŒ€ 40ê°œë¡œ ì œí•œ
            else:
                print(f"   âš ï¸  Keyword filtering failed, using initial keywords only")
                return initial_keywords
                
        except Exception as e:
            print(f"   âš ï¸  Keyword filtering error: {e}, using initial keywords only")
            return initial_keywords
    
    def _further_expand_keywords(self, keywords: List[str]) -> List[str]:
        """í‚¤ì›Œë“œ ìœ ì§€ (ì¬ì‹œë„ ì‹œì—ë„ í™•ì¥ ì—†ìŒ)"""
        # ì¬ì‹œë„ ì‹œì—ë„ ë™ì¼í•œ í‚¤ì›Œë“œ ì‚¬ìš© (í™•ì¥ ì—†ìŒ)
        return keywords
    
    async def _check_sufficiency(
        self,
        topic: str,
        initial_keywords: List[str],
        expanded_keywords: List[str],
        arxiv_data: Optional[Dict[str, Any]],
        rag_results: Optional[Dict[str, Any]],
        news_data: Optional[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """ë°ì´í„° ì¶©ë¶„ì„± íŒë‹¨ (ë³´ê³ ì„œ ì‘ì„± ê°€ëŠ¥ ì—¬ë¶€)"""
        try:
            # ë°ì´í„° ìš”ì•½
            arxiv_count = arxiv_data.get("total_count", 0) if arxiv_data else 0
            arxiv_companies = list(arxiv_data.get("company_stats", {}).keys()) if arxiv_data else []
            
            rag_count = rag_results.get("total_results", 0) if rag_results else 0
            rag_queries = rag_results.get("queries", []) if rag_results else []
            
            news_count = news_data.get("total_articles", 0) if news_data else 0
            news_sources_count = news_data.get("unique_sources", 0) if news_data else 0
            
            # Prompt
            prompt = SUFFICIENCY_CHECK_PROMPT.format(
                topic=topic,
                keywords=", ".join(expanded_keywords or initial_keywords),
                arxiv_count=arxiv_count,
                arxiv_date_range="2022-2025",
                arxiv_companies=", ".join(arxiv_companies[:10]),
                arxiv_keywords=", ".join((expanded_keywords or initial_keywords)[:10]),
                rag_count=rag_count,
                rag_queries=", ".join(rag_queries),
                news_count=news_count,
                news_sources=news_sources_count,
                news_date_range="3 years"
            )
            
            # LLM í˜¸ì¶œ
            response = await self.sufficiency_llm.ainvoke(prompt)
            content = response.content
            
            # JSON íŒŒì‹±
            if "```json" in content:
                content = content.split("```json")[1].split("```")[0].strip()
            elif "```" in content:
                content = content.split("```")[1].split("```")[0].strip()
            
            result = json.loads(content)
            return result
        
        except Exception as e:
            print(f"   âŒ ì¶©ë¶„ì„± íŒë‹¨ ì—ëŸ¬: {e}")
            
            # ê¸°ë³¸ íŒë‹¨
            arxiv_ok = (arxiv_data.get("total_count", 0) if arxiv_data else 0) >= 30
            rag_ok = (rag_results.get("total_results", 0) if rag_results else 0) >= 10
            news_ok = (news_data.get("total_articles", 0) if news_data else 0) >= 20
            
            sufficient = arxiv_ok and rag_ok and news_ok
            
            return {
                "sufficient": sufficient,
                "overall_score": 0.7 if sufficient else 0.5,
                "section_scores": {
                    "section_2": 0.7 if arxiv_ok else 0.3,
                    "section_3": 0.7 if news_ok else 0.3,
                    "section_4": 0.7 if rag_ok else 0.3,
                    "citation": 0.7,
                    "balance": 0.7 if sufficient else 0.5
                },
                "missing_areas": [],
                "recommendations": [],
                "reasoning": "Default judgment"
            }
    
