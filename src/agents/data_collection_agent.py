"""
Data Collection Agent (ReAct Architecture)

ë°ì´í„° ìˆ˜ì§‘ì„ ë‹´ë‹¹í•˜ëŠ” Agent
- ArXiv ë…¼ë¬¸ ìˆ˜ì§‘ ë° í‚¤ì›Œë“œ ì¶”ì¶œ (ë¨¼ì € ì‹¤í–‰)
- RAG + Newsë¥¼ ReAct Agentê°€ ììœ¨ì ìœ¼ë¡œ ì‚¬ìš©
- ë°ì´í„° ì¶©ë¶„ì„± íŒë‹¨ (ë³´ê³ ì„œ ì‘ì„± ê°€ëŠ¥ ì—¬ë¶€)
- Citation ì •ë³´ ìˆ˜ì§‘
"""

import json
import time
from typing import List, Any, Dict, Optional, Tuple
from datetime import datetime
from enum import Enum

from langchain_core.language_models import BaseChatModel
from langchain_openai import ChatOpenAI
from langchain_classic.agents import AgentExecutor, create_react_agent
from langchain_core.prompts import PromptTemplate

from src.agents.base.base_agent import BaseAgent
from src.agents.base.agent_config import AgentConfig
from src.graph.state import PipelineState, WorkflowStatus
from src.core.settings import Settings
from src.core.models.citation_model import (
    CitationCollection
)
from config.prompts.data_collections_prompts import (
    SUFFICIENCY_CHECK_PROMPT,
    SYSTEM_PAPER_KEYWORD_SUMMARY_PROMPT
)


class CollectionConstants:
    """Constants for data collection"""
    MAX_ATTEMPTS = 3
    MAX_AGENT_ITERATIONS = 25
    MAX_EXECUTION_TIME = 1200  # 20 minutes
    SUFFICIENCY_MODEL = "gpt-4o"
    SUFFICIENCY_TEMPERATURE = 0.3
    ARXIV_MAX_RETRIES = 3
    RETRY_SLEEP_SECONDS = 3

    # Arxiv settings
    DEFAULT_ARXIV_CATEGORIES = "cs.RO,cs.AI"
    MAX_RESULTS_PER_KEYWORD = 100
    YEARS_BACK = 3

    # Keyword settings
    MAX_RECENT_PAPERS_ANALYSIS = 20
    MIN_COMPANY_MENTIONS = 2
    MAX_RAW_KEYWORDS_FOR_LLM = 100
    MAX_PAPER_TITLES_FOR_LLM = 15


class DataCollectionAgent(BaseAgent):
    """
    Data Collection Agent (ReAct Architecture)
    
    Workflow:
    1. ArXiv ë…¼ë¬¸ ìˆ˜ì§‘ (ë¨¼ì € ì‹¤í–‰, tool ì•„ë‹˜)
    2. ë…¼ë¬¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
    3. ReAct Agentê°€ RAG + News toolì„ ììœ¨ì ìœ¼ë¡œ ì‚¬ìš©
       - í™•ì¥ëœ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰
       - ì¶©ë¶„í•  ë•Œê¹Œì§€ ë°˜ë³µ
    4. ë°ì´í„° ì¶©ë¶„ì„± íŒë‹¨ (ë³´ê³ ì„œ ì‘ì„± ê°€ëŠ¥ ì—¬ë¶€)
    5. ë¶€ì¡±í•˜ë©´ Agentê°€ ì¶”ê°€ ìˆ˜ì§‘
    
    Features:
    - ArXiv: ì´ˆê¸° ì‹¤í–‰ìœ¼ë¡œ ê¸°ìˆ  landscape íŒŒì•…
    - ReAct Agent: RAG + News toolë§Œ ì‚¬ìš©
    - Citation ìë™ ìƒì„±
    - ì¶©ë¶„ì„± íŒë‹¨: ë³´ê³ ì„œ ëª©ì°¨ ê¸°ì¤€
    """
    
    def __init__(
        self,
        llm: BaseChatModel,
        tools: List[Any],
        config: AgentConfig,
        raw_tools: Optional[List[Any]] = None,
        settings: Optional[Settings] = None
    ):
        super().__init__(llm, tools, config)
        self.settings = settings or Settings()
        self._raw_tools = raw_tools or []
        self._arxiv_tool: Optional[Any] = None
        self._rag_tool: Optional[Any] = None
        self._news_tool: Optional[Any] = None
        self._initialize_tools()

        # Sufficiency check LLM
        self._sufficiency_llm = ChatOpenAI(
            model=CollectionConstants.SUFFICIENCY_MODEL,
            temperature=CollectionConstants.SUFFICIENCY_TEMPERATURE
        )

        # ReAct Agent
        self._agent_executor: Optional[AgentExecutor] = None
        self._setup_react_agent()

    def _initialize_tools(self) -> None:
        """Initialize raw tools by index"""
        if len(self._raw_tools) >= 3:
            self._arxiv_tool = self._raw_tools[0]
            self._rag_tool = self._raw_tools[1]
            self._news_tool = self._raw_tools[2]
    
    def _setup_react_agent(self) -> None:
        """Setup ReAct Agent with RAG + News tools only"""
        react_template = """You are a data collection specialist for AI-Robotics trend reports.

You have already collected ArXiv papers and extracted technical keywords.
Now use these tools to gather market trends and expert forecasts:

{tools}

STRICT FORMAT:

Question: the input question you must answer
Thought: you should always think about what to do next
Action: the action to take, should be one of [{tool_names}]
Action Input: the input to the action (valid JSON)
Observation: the result of the action
... (repeat Thought/Action/Action Input/Observation as needed)
Thought: I have collected sufficient data
Final Answer: summary of collected data

Guidelines:
1. Use search_reference_documents for expert forecasts and technology trends
2. Use search_tech_news for company activities and real-world applications
3. Try different keyword combinations to get diverse sources
4. Collect at least 10 RAG documents and 20 news articles
5. When you have enough data, provide Final Answer

Begin!

Question: {input}
Thought:{agent_scratchpad}"""

        react_prompt = PromptTemplate(
            template=react_template,
            input_variables=["input", "tools", "tool_names", "agent_scratchpad"]
        )

        react_agent = create_react_agent(
            llm=self.llm,
            tools=self.tools,
            prompt=react_prompt
        )

        self._agent_executor = AgentExecutor(
            agent=react_agent,
            tools=self.tools,
            verbose=True,
            max_iterations=CollectionConstants.MAX_AGENT_ITERATIONS,
            max_execution_time=CollectionConstants.MAX_EXECUTION_TIME,
            handle_parsing_errors=True,
            return_intermediate_steps=True
        )
    
    async def execute(self, state: PipelineState) -> PipelineState:
        """
        Data Collection ì‹¤í–‰
        
        Workflow:
        1. ArXiv ë…¼ë¬¸ ìˆ˜ì§‘ (ì§ì ‘ ì‹¤í–‰)
        2. í‚¤ì›Œë“œ í™•ì¥
        3. ReAct Agentë¡œ RAG + News ìˆ˜ì§‘
        4. ì¶©ë¶„ì„± íŒë‹¨
        5. ë¶€ì¡±í•˜ë©´ ì¬ì‹œë„
        """
        print(f"\n{'='*60}")
        print(f"ğŸ“š Data Collection Agent ì‹¤í–‰ ì¤‘...")
        print(f"{'='*60}\n")
        
        # Stateì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        planning_output = state.get("planning_output")
        if not planning_output:
            raise ValueError("planning_outputì´ Stateì— ì—†ìŠµë‹ˆë‹¤.")
        
        topic = planning_output.topic
        keywords = state.get("keywords", [])
        
        print(f"ğŸ“ ì£¼ì œ: {topic}")
        print(f"ğŸ”‘ ì´ˆê¸° í‚¤ì›Œë“œ: {', '.join(keywords)}\n")
        
        # ìµœëŒ€ ì¬ì‹œë„
        max_attempts = 3
        attempt = 0
        
        # ìˆ˜ì§‘ ê²°ê³¼
        arxiv_data = None
        rag_results = None
        news_data = None
        expanded_keywords = []
        citations = CitationCollection()
        
        while attempt < max_attempts:
            attempt += 1
            print(f"\n{'='*60}")
            print(f"ğŸ”„ ë°ì´í„° ìˆ˜ì§‘ ì‹œë„ {attempt}/{max_attempts}")
            print(f"{'='*60}\n")
            
            try:
                # ========================================
                # Step 1: ArXiv ë…¼ë¬¸ ìˆ˜ì§‘ (ë¨¼ì € ì‹¤í–‰)
                # ========================================
                if attempt == 1:  # ì²« ì‹œë„ì—ë§Œ ArXiv ìˆ˜ì§‘
                    print(f"ğŸ“„ Step 1: ArXiv ë…¼ë¬¸ ìˆ˜ì§‘ ì¤‘...")
                    arxiv_data = await self._collect_arxiv(keywords, planning_output)
                    
                    if arxiv_data and arxiv_data.get("total_count", 0) > 0:
                        print(f"   âœ… ArXiv ìˆ˜ì§‘ ì™„ë£Œ: {arxiv_data['total_count']}í¸")
                        # Toolì—ì„œ ìƒì„±í•œ citations ê°€ì ¸ì˜¤ê¸°
                        tool_citations = arxiv_data.get("citations", [])
                        citations.arxiv_citations.extend(tool_citations)
                        print(f"   âœ… ArXiv Citation: {len(tool_citations)}ê°œ\n")
                    else:
                        print(f"   âš ï¸  ArXiv ë°ì´í„° ì—†ìŒ\n")
                    
                    # Step 2: í‚¤ì›Œë“œ ì¶”ì¶œ
                    print(f"ğŸ”‘ Step 2: ë…¼ë¬¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...")
                    expanded_keywords = await self._expand_keywords(arxiv_data, keywords)
                    print(f"   âœ… ì¶”ì¶œëœ í‚¤ì›Œë“œ ({len(expanded_keywords)}ê°œ): {', '.join(expanded_keywords[:10])}{'...' if len(expanded_keywords) > 10 else ''}\n")
                
                # ========================================
                # Step 3: ReAct Agentë¡œ RAG + News ìˆ˜ì§‘
                # ========================================
                print(f"ğŸ¤– Step 3: ReAct Agent ì‹œì‘ (RAG + News)...\n")
                
                # Agentì—ê²Œ ì¤„ ì§ˆë¬¸ ìƒì„±
                question = self._generate_agent_question(
                    topic, expanded_keywords, arxiv_data, attempt
                )
                
                # ReAct Agent ì‹¤í–‰
                result = await self._agent_executor.ainvoke({
                    "input": question
                })
                
                print(f"\nâœ… ReAct Agent ì™„ë£Œ!\n")
                
                # Wrapper ìºì‹œì—ì„œ Agentê°€ ìˆ˜ì§‘í•œ ë°ì´í„° ì¶”ì¶œ (ì¬ê²€ìƒ‰ ì—†ìŒ!)
                print(f"ğŸ“Š Agent ìˆ˜ì§‘ ë°ì´í„° ì¶”ì¶œ ì¤‘ (ìºì‹œ ì‚¬ìš©)...\n")
                
                rag_results, news_data = self._extract_data_from_cache(
                    topic, expanded_keywords
                )
                
                # Citation ì¶”ê°€ (Toolì—ì„œ ìƒì„±í•œ ê²ƒ ê°€ì ¸ì˜¤ê¸°)
                if rag_results:
                    tool_citations = rag_results.get("citations", [])
                    citations.rag_citations.extend(tool_citations)
                    print(f"   âœ… RAG Citation: {len(tool_citations)}ê°œ")
                
                if news_data:
                    tool_citations = news_data.get("citations", [])
                    citations.news_citations.extend(tool_citations)
                    print(f"   âœ… News Citation: {len(tool_citations)}ê°œ\n")
                
                # ========================================
                # Step 4: ë°ì´í„° ì¶©ë¶„ì„± íŒë‹¨
                # ========================================
                print(f"ğŸ” Step 4: ë°ì´í„° ì¶©ë¶„ì„± íŒë‹¨ ì¤‘...")
                sufficiency_result = await self._check_sufficiency(
                    topic, keywords, expanded_keywords,
                    arxiv_data, rag_results, news_data
                )
                
                print(f"   ğŸ“Š ì¶©ë¶„ì„± ì ìˆ˜: {sufficiency_result.get('overall_score', 0):.2f}")
                print(f"   íŒì •: {'âœ… ì¶©ë¶„í•¨' if sufficiency_result.get('sufficient', False) else 'âš ï¸  ë¶€ì¡±í•¨'}\n")
                
                # ì¶©ë¶„í•˜ë©´ ì¢…ë£Œ
                if sufficiency_result.get("sufficient", False):
                    print(f"âœ… ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ! (ë³´ê³ ì„œ ì‘ì„± ê°€ëŠ¥)\n")
                    break
                
                # ë¶€ì¡±í•˜ë©´ ì¬ì‹œë„
                if attempt < max_attempts:
                    print(f"âš ï¸  ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ì¶”ê°€ ìˆ˜ì§‘ì„ ì‹œë„í•©ë‹ˆë‹¤.")
                    print(f"   ë¶€ì¡±í•œ ì˜ì—­: {', '.join(sufficiency_result.get('missing_areas', []))}")
                    print(f"   ê¶Œì¥ì‚¬í•­: {', '.join(sufficiency_result.get('recommendations', []))}\n")
                    
                    # í‚¤ì›Œë“œ ì¶”ê°€ í™•ì¥
                    expanded_keywords = self._further_expand_keywords(expanded_keywords)
                else:
                    print(f"âš ï¸  ìµœëŒ€ ì‹œë„ íšŸìˆ˜ ë„ë‹¬. í˜„ì¬ ë°ì´í„°ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.\n")
            
            except Exception as e:
                print(f"âŒ ë°ì´í„° ìˆ˜ì§‘ ì¤‘ ì—ëŸ¬: {e}")
                print(f"error")
                
                if attempt >= max_attempts:
                    print(f"\nâš ï¸  ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ë„ë‹¬. ìˆ˜ì§‘ëœ ë°ì´í„°ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.\n")
                    break
                
                print(f"ğŸ”„ ì¬ì‹œë„ ì¤‘...\n")
                time.sleep(2)
                continue
        
        # ê²°ê³¼ ìš”ì•½
        print(f"\n{'='*60}")
        print(f"ğŸ“Š ë°ì´í„° ìˆ˜ì§‘ ê²°ê³¼ ìš”ì•½")
        print(f"{'='*60}")
        print(f"ğŸ“„ ArXiv ë…¼ë¬¸: {arxiv_data.get('total_count', 0) if arxiv_data else 0}í¸")
        print(f"ğŸ“– RAG ê²°ê³¼: {rag_results.get('total_results', 0) if rag_results else 0}ê°œ")
        print(f"ğŸ“° ë‰´ìŠ¤ ê¸°ì‚¬: {news_data.get('total_articles', 0) if news_data else 0}ê°œ")
        print(f"ğŸ”‘ ì¶”ì¶œ í‚¤ì›Œë“œ: {len(expanded_keywords)}ê°œ")
        print(f"ğŸ“š ì¸ìš© ì •ë³´ (ì¶œì²˜):")
        print(f"   - ArXiv ë…¼ë¬¸: {len(citations.arxiv_citations)}ê°œ")
        print(f"   - ë‰´ìŠ¤ ê¸°ì‚¬: {len(citations.news_citations)}ê°œ")
        print(f"   - RAG ë¬¸ì„œ: {len(citations.rag_citations)}ê°œ")
        print(f"   - ì´ {len(citations.get_all_citations())}ê°œ ì¶œì²˜")
        print(f"{'='*60}\n")
        
        # State ì—…ë°ì´íŠ¸
        state["arxiv_data"] = arxiv_data or {}
        state["news_data"] = news_data or {}
        state["rag_results"] = rag_results or {}
        state["expanded_keywords"] = expanded_keywords
        state["citations"] = citations
        state["status"] = WorkflowStatus.DATA_COLLECTION_COMPLETE.value
        
        return state
    
    def _extract_data_from_cache(
        self,
        topic: str,
        expanded_keywords: List[str]
    ) -> tuple:
        """
        Wrapper ìºì‹œì—ì„œ Agentê°€ ìˆ˜ì§‘í•œ ë°ì´í„° ì¶”ì¶œ (ì¬ê²€ìƒ‰ ë¶ˆí•„ìš”!)
        
        Returns:
            (rag_results, news_data)
        """
        # RAG wrapperì—ì„œ ìºì‹œ ê°€ì ¸ì˜¤ê¸°
        rag_wrapper = None
        news_wrapper = None
        
        for tool in self.tools:
            if 'reference' in tool.name.lower() or 'document' in tool.name.lower():
                rag_wrapper = tool
            elif 'news' in tool.name.lower():
                news_wrapper = tool
        
        # RAG ìºì‹œ ë°ì´í„° í†µí•©
        rag_documents = []
        rag_citations = []
        seen_contents = set()
        seen_rag_citations = set()  # Citation ì¤‘ë³µ ì œê±°ìš©
        
        if rag_wrapper and hasattr(rag_wrapper, 'get_cached_data'):
            cached_rag = rag_wrapper.get_cached_data()
            print(f"   ğŸ“– RAG ìºì‹œ: {len(cached_rag)}ë²ˆ ê²€ìƒ‰ ìˆ˜í–‰ë¨")
            
            for cache_entry in cached_rag:
                entry_docs = cache_entry.get("documents", [])
                entry_citations = cache_entry.get("citations", [])
                
                # Documentì™€ Citationì„ í•¨ê»˜ ì²˜ë¦¬ (1:1 ë§¤ì¹­ ê°€ì •)
                for i, doc in enumerate(entry_docs):
                    content = doc.get("content", "")
                    if content and content not in seen_contents:
                        seen_contents.add(content)
                        rag_documents.append(doc)
                        
                        # ëŒ€ì‘í•˜ëŠ” citation ì¶”ê°€ (ì¤‘ë³µ ì²´í¬)
                        if i < len(entry_citations):
                            citation = entry_citations[i]
                            citation_key = citation.full_citation if hasattr(citation, 'full_citation') else str(citation)
                            if citation_key not in seen_rag_citations:
                                seen_rag_citations.add(citation_key)
                                rag_citations.append(citation)
        
        # News ìºì‹œ ë°ì´í„° í†µí•©
        news_articles = []
        news_citations = []
        seen_urls = set()
        seen_news_citations = set()  # Citation ì¤‘ë³µ ì œê±°ìš©
        
        if news_wrapper and hasattr(news_wrapper, 'get_cached_data'):
            cached_news = news_wrapper.get_cached_data()
            print(f"   ğŸ“° News ìºì‹œ: {len(cached_news)}ë²ˆ ê²€ìƒ‰ ìˆ˜í–‰ë¨")
            
            for cache_entry in cached_news:
                entry_articles = cache_entry.get("articles", [])
                entry_citations = cache_entry.get("citations", [])
                
                # Articleê³¼ Citationì„ í•¨ê»˜ ì²˜ë¦¬ (1:1 ë§¤ì¹­ ê°€ì •)
                for i, article in enumerate(entry_articles):
                    url = article.get("url", "")
                    if url and url not in seen_urls:
                        seen_urls.add(url)
                        news_articles.append(article)
                        
                        # ëŒ€ì‘í•˜ëŠ” citation ì¶”ê°€ (ì¤‘ë³µ ì²´í¬)
                        if i < len(entry_citations):
                            citation = entry_citations[i]
                            citation_key = citation.full_citation if hasattr(citation, 'full_citation') else str(citation)
                            if citation_key not in seen_news_citations:
                                seen_news_citations.add(citation_key)
                                news_citations.append(citation)
        
        # RAG ê²°ê³¼ ì •ë¦¬
        rag_results = {
            "query": topic,
            "search_type": "agent_collected_cached",
            "total_results": len(rag_documents),
            "documents": rag_documents,
            "citations": rag_citations  # Citations ì¶”ê°€
        } if rag_documents else None
        
        # News ê²°ê³¼ ì •ë¦¬
        news_data = {
            "keywords": expanded_keywords,
            "date_range": "3 years",
            "total_articles": len(news_articles),
            "unique_sources": len(set(a.get("source", "") for a in news_articles)),
            "articles": news_articles,
            "citations": news_citations  # Citations ì¶”ê°€
        } if news_articles else None
        
        # ìµœì¢… í†µê³„ ì¶œë ¥
        if rag_results:
            print(f"   âœ… RAG ë°ì´í„°: {len(rag_documents)}ê°œ ë¬¸ì„œ ì¶”ì¶œë¨")
        if news_data:
            print(f"   âœ… News ë°ì´í„°: {len(news_articles)}ê°œ ê¸°ì‚¬ ì¶”ì¶œë¨\n")
        
        return rag_results, news_data
    
    def _generate_agent_question(
        self,
        topic: str,
        keywords: List[str],
        arxiv_data: Optional[Dict],
        attempt: int
    ) -> str:
        """ReAct Agentì—ê²Œ ì¤„ ì§ˆë¬¸ ìƒì„±"""
        paper_count = arxiv_data.get("total_count", 0) if arxiv_data else 0
        
        if attempt == 1:
            # ë¡œë´‡/ê¸°ìˆ  ê´€ë ¨ í‚¤ì›Œë“œë§Œ ì„ ë³„
            question = f"""Collect comprehensive data for a ROBOTICS/AUTOMATION technology trend report on "{topic}".

I have already collected {paper_count} ArXiv papers and extracted these ROBOTICS-SPECIFIC keywords:
{', '.join(keywords[:])}

Your task:
1. Use search_reference_documents to find expert forecasts about ROBOTICS/AI technologies
   - Search for: robotics trends, AI in automation, 5-year predictions for robots
   - Look for: industrial robotics, service robots, autonomous systems
   - Get: future technology forecasts, market adoption predictions
   - Use TECHNOLOGY keywords from the list above
   - Target: At least 10 relevant documents

2. Use search_tech_news to find recent ROBOTICS/AI news
   - Use ONLY technology keywords (e.g., "collaborative robots", "adaptive welding")
   - Focus on: robot deployments, new robot products, automation innovations
   - Search for: specific companies + technology combinations
   - DO NOT use generic terms like "market activities", "announcements"
   - Target: At least 20 diverse articles about robot technologies

**IMPORTANT:** Use SPECIFIC technology keywords, not generic business terms!
When you have enough data, provide a final summary."""
        
        else:
            question = f"""Continue collecting more data for "{topic}".

Current status: Need more diverse sources.

Use the keywords: {', '.join(keywords[:10])}

Focus on:
- Finding different sources and perspectives
- Covering various aspects of the technology
- Getting recent market activities

Continue until you have comprehensive coverage."""
        
        return question
    
    async def _collect_arxiv(
        self,
        keywords: List[str],
        planning_output: Any
    ) -> Optional[Dict[str, Any]]:
        """ArXiv ë…¼ë¬¸ ìˆ˜ì§‘ (í‚¤ì›Œë“œë³„ ë³‘ë ¬ ê²€ìƒ‰)"""
        for retry in range(CollectionConstants.ARXIV_MAX_RETRIES):
            try:
                if not self._arxiv_tool:
                    print("   âš ï¸  ArXiv Toolì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    return None

                # Get categories from collection plan
                categories_str = planning_output.collection_plan.arxiv.categories
                if categories_str.lower() == "all":
                    categories_str = CollectionConstants.DEFAULT_ARXIV_CATEGORIES

                print(f"   ğŸ“š ì¹´í…Œê³ ë¦¬: {categories_str}")
                print(f"   ğŸ”‘ í‚¤ì›Œë“œ: {len(keywords)}ê°œ (ê° í‚¤ì›Œë“œë‹¹ ë³‘ë ¬ ê²€ìƒ‰)")
                print(f"   ğŸ¯ í‚¤ì›Œë“œë‹¹ ìµœëŒ€: {CollectionConstants.MAX_RESULTS_PER_KEYWORD}í¸\n")

                # Search ArXiv papers in parallel
                result = self._arxiv_tool.search_by_keywords_parallel(
                    keywords=keywords,
                    categories=categories_str,
                    max_results_per_keyword=CollectionConstants.MAX_RESULTS_PER_KEYWORD,
                    years_back=CollectionConstants.YEARS_BACK
                )

                if result and result.get("total_count", 0) > 0:
                    return result
                else:
                    print(f"   âš ï¸  ë…¼ë¬¸ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì¬ì‹œë„ {retry + 1}/{CollectionConstants.ARXIV_MAX_RETRIES}")
                    if retry < CollectionConstants.ARXIV_MAX_RETRIES - 1:
                        time.sleep(CollectionConstants.RETRY_SLEEP_SECONDS)
                        continue
                    return result

            except Exception as e:
                print(f"   âŒ ArXiv ìˆ˜ì§‘ ì—ëŸ¬ (ì‹œë„ {retry + 1}/{CollectionConstants.ARXIV_MAX_RETRIES}): {e}")
                if retry < CollectionConstants.ARXIV_MAX_RETRIES - 1:
                    print(f"   ğŸ”„ {CollectionConstants.RETRY_SLEEP_SECONDS}ì´ˆ í›„ ì¬ì‹œë„...")
                    time.sleep(CollectionConstants.RETRY_SLEEP_SECONDS)
                    continue
                else:
                    print(f"   error")
                    return None

        return None
    
    
    async def _expand_keywords(
        self,
        arxiv_data: Optional[Dict[str, Any]],
        initial_keywords: List[str]
    ) -> List[str]:
        """
        ArXiv ë…¼ë¬¸ì—ì„œ emerging/íŠ¹ì´í•œ ê¸°ìˆ  í‚¤ì›Œë“œ ì¶”ì¶œ
        
        ëª©í‘œ: 5ë…„ í›„ íŠ¸ë Œë“œë¥¼ ì˜ˆì¸¡í•  ìˆ˜ ìˆëŠ” ìƒˆë¡­ê³  êµ¬ì²´ì ì¸ í‚¤ì›Œë“œ ë°œêµ´
        
        1. ë…¼ë¬¸ ì œëª©/ì´ˆë¡ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
        2. ê¸°ì—… ì •ë³´ ì¶”ì¶œ
        3. LLMì´ emerging/specific í‚¤ì›Œë“œ ì„ ë³„
           - ì¼ë°˜ì  í‚¤ì›Œë“œ(machine learning) ì œì™¸
           - êµ¬ì²´ì /ìƒˆë¡œìš´ í‚¤ì›Œë“œ(neuromorphic computing) ìš°ì„ 
        """
        if not arxiv_data or not arxiv_data.get("papers"):
            return initial_keywords  # ë…¼ë¬¸ì´ ì—†ìœ¼ë©´ ì´ˆê¸° í‚¤ì›Œë“œ ì‚¬ìš©
        
        # 1. ë…¼ë¬¸ì—ì„œ ì¶”ì¶œëœ í‚¤ì›Œë“œ ìˆ˜ì§‘
        raw_keywords = set()
        for paper in arxiv_data["papers"]:
            paper_keywords = paper.get("keywords", [])
            for kw in paper_keywords:
                raw_keywords.add(kw)
        
        # 2. ë…¼ë¬¸ì—ì„œ ì–¸ê¸‰ëœ ê¸°ì—… ì¶”ì¶œ
        companies_mentioned = set()
        if "companies_mentioned" in arxiv_data:
            for company, count in arxiv_data["companies_mentioned"].items():
                if count >= CollectionConstants.MIN_COMPANY_MENTIONS:
                    companies_mentioned.add(company)
        
        # 3. ë…¼ë¬¸ ì œëª© ë¶„ì„ (ìµœì‹  ê¸°ìˆ  ê²½í–¥ íŒŒì•…)
        recent_papers_info = []
        if arxiv_data.get("papers"):
            # ìµœì‹  ë…¼ë¬¸ë§Œ ë¶„ì„ (ìµœì‹  íŠ¸ë Œë“œ)
            for paper in arxiv_data["papers"][:CollectionConstants.MAX_RECENT_PAPERS_ANALYSIS]:
                recent_papers_info.append({
                    "title": paper.get("title", ""),
                    "year": paper.get("published", "")[:4]  # YYYYë§Œ ì¶”ì¶œ
                })
        
        # 4. ëª¨ë“  í›„ë³´ í‚¤ì›Œë“œ ê²°í•©
        all_candidates = list(raw_keywords) + list(companies_mentioned)
        
        # ë…¼ë¬¸ í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ ì´ˆê¸° í‚¤ì›Œë“œ ì‚¬ìš©
        if not all_candidates:
            return initial_keywords
        
        # 5. LLMìœ¼ë¡œ emerging/specific í‚¤ì›Œë“œ ì„ ë³„ â­
        filtered_keywords = await self._filter_emerging_keywords(
            initial_keywords=initial_keywords,
            raw_keywords=all_candidates,
            companies=list(companies_mentioned),
            recent_papers=recent_papers_info
        )
        
        return filtered_keywords
    
    async def _filter_emerging_keywords(
        self,
        initial_keywords: List[str],
        raw_keywords: List[str],
        companies: Optional[List[str]] = None,
        recent_papers: Optional[List[Dict]] = None
    ) -> List[str]:
        """
        LLMì„ ì‚¬ìš©í•˜ì—¬ emerging/specific í‚¤ì›Œë“œ ì„ ë³„
        
        ëª©í‘œ: 5ë…„ í›„ íŠ¸ë Œë“œ ì˜ˆì¸¡ì„ ìœ„í•œ ìƒˆë¡­ê³  êµ¬ì²´ì ì¸ í‚¤ì›Œë“œ ë°œêµ´
        
        Args:
            initial_keywords: ì‚¬ìš©ì ì§ˆë¬¸ ê¸°ë°˜ ì´ˆê¸° í‚¤ì›Œë“œ
            raw_keywords: arXiv ë…¼ë¬¸ì—ì„œ ì¶”ì¶œëœ ëª¨ë“  í‚¤ì›Œë“œ
            companies: ë…¼ë¬¸ì—ì„œ ì–¸ê¸‰ëœ ê¸°ì—… ë¦¬ìŠ¤íŠ¸
            recent_papers: ìµœì‹  ë…¼ë¬¸ ì œëª© ë¦¬ìŠ¤íŠ¸
        
        Returns:
            emerging/specific í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ (ê¸°ì—… í¬í•¨)
        """
        companies = companies or []
        recent_papers = recent_papers or []
        
        from langchain_core.prompts import ChatPromptTemplate
        from langchain_core.output_parsers import StrOutputParser
        
        # ìµœì‹  ë…¼ë¬¸ ì œëª© ìš”ì•½
        paper_titles = "\n".join([f"- ({p['year']}) {p['title']}" for p in recent_papers[:CollectionConstants.MAX_PAPER_TITLES_FOR_LLM]])

        filter_prompt = ChatPromptTemplate.from_messages(SYSTEM_PAPER_KEYWORD_SUMMARY_PROMPT)
        
        try:
            chain = filter_prompt | self.llm | StrOutputParser()
            response = await chain.ainvoke({
                "initial_keywords": ", ".join(initial_keywords),
                "paper_titles": paper_titles if paper_titles else "No recent papers available",
                "raw_keywords": ", ".join(raw_keywords[:CollectionConstants.MAX_RAW_KEYWORDS_FOR_LLM]),
                "companies": ", ".join(companies) if companies else "None mentioned"
            })
            
            # JSON íŒŒì‹±
            import re
            json_match = re.search(r'\[.*?\]', response, re.DOTALL)
            if json_match:
                filtered = json.loads(json_match.group(0))
                # ì´ˆê¸° í‚¤ì›Œë“œëŠ” í•­ìƒ í¬í•¨
                final_keywords = list(set(initial_keywords + filtered))
                
                # í†µê³„ ì¶œë ¥
                tech_keywords = [kw for kw in filtered if kw not in companies]
                company_keywords = [kw for kw in filtered if kw in companies]
                
                print(f"   ğŸ” Emerging keyword extraction:")
                print(f"      â€¢ Raw candidates: {len(raw_keywords)}")
                print(f"      â€¢ Emerging/specific tech: {len(tech_keywords)}")
                print(f"      â€¢ Companies identified: {len(company_keywords)}")
                print(f"      â€¢ Total (with initial): {len(final_keywords)}")
                print(f"      â€¢ Top emerging tech: {', '.join(tech_keywords[:5])}...")
                
                if company_keywords:
                    print(f"      â€¢ Companies: {', '.join(company_keywords[:5])}{'...' if len(company_keywords) > 5 else ''}")
                
                return sorted(final_keywords[:40])  # ìµœëŒ€ 40ê°œë¡œ ì œí•œ
            else:
                print(f"   âš ï¸  Keyword filtering failed, using initial keywords only")
                return initial_keywords
                
        except Exception as e:
            print(f"   âš ï¸  Keyword filtering error: {e}, using initial keywords only")
            return initial_keywords
    
    def _further_expand_keywords(self, keywords: List[str]) -> List[str]:
        """í‚¤ì›Œë“œ ìœ ì§€ (ì¬ì‹œë„ ì‹œì—ë„ í™•ì¥ ì—†ìŒ)"""
        # ì¬ì‹œë„ ì‹œì—ë„ ë™ì¼í•œ í‚¤ì›Œë“œ ì‚¬ìš© (í™•ì¥ ì—†ìŒ)
        return keywords
    
    async def _check_sufficiency(
        self,
        topic: str,
        initial_keywords: List[str],
        expanded_keywords: List[str],
        arxiv_data: Optional[Dict[str, Any]],
        rag_results: Optional[Dict[str, Any]],
        news_data: Optional[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """ë°ì´í„° ì¶©ë¶„ì„± íŒë‹¨ (ë³´ê³ ì„œ ì‘ì„± ê°€ëŠ¥ ì—¬ë¶€)"""
        try:
            # ë°ì´í„° ìš”ì•½
            arxiv_count = arxiv_data.get("total_count", 0) if arxiv_data else 0
            arxiv_companies = list(arxiv_data.get("company_stats", {}).keys()) if arxiv_data else []
            
            rag_count = rag_results.get("total_results", 0) if rag_results else 0
            rag_queries = rag_results.get("queries", []) if rag_results else []
            
            news_count = news_data.get("total_articles", 0) if news_data else 0
            news_sources_count = news_data.get("unique_sources", 0) if news_data else 0
            
            # Prompt
            prompt = SUFFICIENCY_CHECK_PROMPT.format(
                topic=topic,
                keywords=", ".join(expanded_keywords or initial_keywords),
                arxiv_count=arxiv_count,
                arxiv_date_range="2022-2025",
                arxiv_companies=", ".join(arxiv_companies[:10]),
                arxiv_keywords=", ".join((expanded_keywords or initial_keywords)[:10]),
                rag_count=rag_count,
                rag_queries=", ".join(rag_queries),
                news_count=news_count,
                news_sources=news_sources_count,
                news_date_range="3 years"
            )
            
            # LLM í˜¸ì¶œ
            response = await self.sufficiency_llm.ainvoke(prompt)
            content = response.content
            
            # JSON íŒŒì‹±
            if "```json" in content:
                content = content.split("```json")[1].split("```")[0].strip()
            elif "```" in content:
                content = content.split("```")[1].split("```")[0].strip()
            
            result = json.loads(content)
            return result
        
        except Exception as e:
            print(f"   âŒ ì¶©ë¶„ì„± íŒë‹¨ ì—ëŸ¬: {e}")
            
            # ê¸°ë³¸ íŒë‹¨
            arxiv_ok = (arxiv_data.get("total_count", 0) if arxiv_data else 0) >= 30
            rag_ok = (rag_results.get("total_results", 0) if rag_results else 0) >= 10
            news_ok = (news_data.get("total_articles", 0) if news_data else 0) >= 20
            
            sufficient = arxiv_ok and rag_ok and news_ok
            
            return {
                "sufficient": sufficient,
                "overall_score": 0.7 if sufficient else 0.5,
                "section_scores": {
                    "section_2": 0.7 if arxiv_ok else 0.3,
                    "section_3": 0.7 if news_ok else 0.3,
                    "section_4": 0.7 if rag_ok else 0.3,
                    "citation": 0.7,
                    "balance": 0.7 if sufficient else 0.5
                },
                "missing_areas": [],
                "recommendations": [],
                "reasoning": "Default judgment"
            }


